{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark Structured API ou Spark SQL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spark SQL est un module Spark pour le traitement structuré des données. Contrairement à l'API Spark RDD de base, les interfaces fournies par Spark SQL fournissent à Spark plus d'informations sur la structure des données et sur le calcul effectué. En interne, Spark SQL utilise ces informations supplémentaires pour effectuer des optimisations supplémentaires. Il existe plusieurs façons d'interagir avec Spark SQL, y compris SQL et l'API Dataset. Lors du calcul d'un résultat, le même moteur d'exécution est utilisé, quelle que soit l'API/langage utilisée pour exprimer le calcul. Cette unification signifie que les développeurs peuvent facilement passer d'une API à l'autre, ce qui constitue la manière la plus naturelle d'exprimer une transformation donnée."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SQL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "L'une des utilisations de Spark SQL est l'exécution de requêtes SQL. Spark SQL peut également être utilisé pour lire des données à partir d'une installation de ruche existante. Lorsque vous exécutez SQL à partir d'un autre langage de programmation, les résultats seront renvoyés sous forme de Dataset/DataFrame. Vous pouvez également interagir avec l'interface SQL en utilisant la ligne de commande ou via JDBC/ODBC."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Un Dataset est une collection de données distribuées. Le Dataset est une nouvelle interface ajoutée dans Spark 1.6 qui offre les avantages des RDD (typage fort, possibilité d'utiliser des fonctions lambda puissantes) avec les avantages du moteur d'exécution optimisé de Spark SQL. Un Dataset peut être construit à partir d'objets JVM et ensuite manipulé à l'aide de transformations fonctionnelles (map, flatMap, filtre, etc.). L'API du Dataset est disponible en Scala et Java. Python ne supporte pas l'API Dataset. Mais en raison de la nature dynamique de Python, de nombreux avantages de l'API Dataset sont déjà disponibles (c'est-à-dire que vous pouvez accéder au champ d'une ligne par son nom naturellement row.columnName). Le cas de R est similaire."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DataFrames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Une DataFrame est un Dataset organisé en colonnes nommées. Il est conceptuellement équivalent à une table dans une base de données relationnelle ou à une trame de données en R/Python, mais avec des optimisations plus riches sous le capot. Les DataFrames peuvent être construites à partir d'un large éventail de sources telles que : des fichiers de données structurées, des tables dans une ruche, des bases de données externes ou des RDD existants. L'API DataFrame est disponible en Scala, Java, Python et R. En Scala et Java, une DataFrame est représentée par un ensemble de lignes de données. Dans l'API Scala, la DataFrame est simplement un alias de type Dataset[Row]. Alors que, dans l'API Java, les utilisateurs doivent utiliser Dataset<Row> pour représenter une DataFrame."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creer un DataFrames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[39m\u001b[36m$ivy.$                                   // Or use any other 2.x version here\n",
       "\u001b[39m\r\n",
       "\u001b[32mimport \u001b[39m\u001b[36m$ivy.$                                // Not required since almond 0.7.0 (will be automatically added when importing spark)\u001b[39m"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import $ivy.`org.apache.spark::spark-sql:2.4.5` // Or use any other 2.x version here\n",
    "import $ivy.`sh.almond::almond-spark:0.10.9` // Not required since almond 0.7.0 (will be automatically added when importing spark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.log4j.{Level, Logger}\n",
       "\u001b[39m"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.log4j.{Level, Logger}\n",
    "Logger.getLogger(\"org\").setLevel(Level.OFF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading spark-stubs\n",
      "Getting spark JARs\n",
      "Creating SparkSession\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<a target=\"_blank\" href=\"http://KHASS-IM:4040\">Spark UI</a>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.sql._\n",
       "\n",
       "\u001b[39m\r\n",
       "\u001b[36mspark\u001b[39m: \u001b[32mSparkSession\u001b[39m = org.apache.spark.sql.SparkSession@5e68f294"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql._\n",
    "\n",
    "val spark = {\n",
    "  NotebookSparkSession.builder()\n",
    "    .master(\"local[*]\")\n",
    "    .getOrCreate()\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vous pouvez importer la bibliothèque `spark implicits` et créer un DataFrame avec la méthode `toDF()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[39m\u001b[36mspark.implicits._\n",
       "\n",
       "\u001b[39m\r\n",
       "\u001b[36mdf\u001b[39m: \u001b[32mDataFrame\u001b[39m = [city: string, country: string ... 1 more field]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import spark.implicits._\n",
    "\n",
    "val df = Seq(\n",
    "    (\"Boston\", \"USA\", 0.67), \n",
    "    (\"Dubai\", \"UAE\", 3.1), \n",
    "    (\"Dakar\", \"Senegal\", 5.28)\n",
    ").toDF(\"city\", \"country\", \"population\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vous pouvez visialiser le contenu d'un DataFrame via la methode `show()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------+----------+\n",
      "|  city|country|population|\n",
      "+------+-------+----------+\n",
      "|Boston|    USA|      0.67|\n",
      "| Dubai|    UAE|       3.1|\n",
      "| Dakar|Senegal|      5.28|\n",
      "+------+-------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ajouter des colonnes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Des colonnes peuvent être ajoutées à un DataFrame avec la méthode `withColumn()`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ajoutons une colonne `is_big_city` au DataFrame qui renvoie vrai si la ville contient plus de\n",
    "un million de personnes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------+----------+-----------+\n",
      "|  city|country|population|is_big_city|\n",
      "+------+-------+----------+-----------+\n",
      "|Boston|    USA|      0.67|      false|\n",
      "| Dubai|    UAE|       3.1|       true|\n",
      "| Dakar|Senegal|      5.28|       true|\n",
      "+------+-------+----------+-----------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.sql.functions.col\n",
       "\n",
       "\u001b[39m\r\n",
       "\u001b[36mdf2\u001b[39m: \u001b[32mDataFrame\u001b[39m = [city: string, country: string ... 2 more fields]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.functions.col\n",
    "\n",
    "val df2 = df.withColumn(\"is_big_city\", col(\"population\") > 1)\n",
    "df2.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Les DataFrames sont immuables, de sorte que la méthode `withColumn()` renvoie un nouveau DataFrame. `withColumn()` ne fait pas muter le DataFrame original. \n",
    "\n",
    "Confirmons-nous que df est toujours le même avec `df.show()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------+----------+\n",
      "|  city|country|population|\n",
      "+------+-------+----------+\n",
      "|Boston|    USA|      0.67|\n",
      "| Dubai|    UAE|       3.1|\n",
      "| Dakar|Senegal|      5.28|\n",
      "+------+-------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "df ne contient pas la colonne `is_big_city`, nous avons donc confirmé que `withColumn()` n'a pas fait muter df."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filtrer des lignes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La méthode `filter()` supprime les lignes d'un DataFrame:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-------+----------+\n",
      "| city|country|population|\n",
      "+-----+-------+----------+\n",
      "|Dubai|    UAE|       3.1|\n",
      "|Dakar|Senegal|      5.28|\n",
      "+-----+-------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.filter(col(\"population\") > 1).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Il est un peu difficile de lire un code avec plusieurs appels de méthode sur la même ligne, alors découpons ce code sur plusieurs lignes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-------+----------+\n",
      "| city|country|population|\n",
      "+-----+-------+----------+\n",
      "|Dubai|    UAE|       3.1|\n",
      "|Dakar|Senegal|      5.28|\n",
      "+-----+-------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df\n",
    " .filter(col(\"population\") > 1)\n",
    " .show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous pouvons également affecter la DataFrame filtrée à une variable distincte plutôt que d'enchaîner les appels de méthode:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-------+----------+\n",
      "| city|country|population|\n",
      "+-----+-------+----------+\n",
      "|Dubai|    UAE|       3.1|\n",
      "|Dakar|Senegal|      5.28|\n",
      "+-----+-------+----------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[36mfilteredDF\u001b[39m: \u001b[32mDataset\u001b[39m[\u001b[32mRow\u001b[39m] = [city: string, country: string ... 1 more field]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val filteredDF = df.filter(col(\"population\") > 1)\n",
    "filteredDF.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Schema"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le schema d'un DataFrame peut être imprimé sur la console avec la méthode `printSchema()`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- city: string (nullable = true)\n",
      " |-- country: string (nullable = true)\n",
      " |-- population: double (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La méthode schema renvoie une représentation codée du `schema` DataFrame:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mres14\u001b[39m: \u001b[32mtypes\u001b[39m.\u001b[32mStructType\u001b[39m = \u001b[33mStructType\u001b[39m(\n",
       "  \u001b[33mStructField\u001b[39m(\u001b[32m\"city\"\u001b[39m, StringType, true, {}),\n",
       "  \u001b[33mStructField\u001b[39m(\u001b[32m\"country\"\u001b[39m, StringType, true, {}),\n",
       "  \u001b[33mStructField\u001b[39m(\u001b[32m\"population\"\u001b[39m, DoubleType, false, {})\n",
       ")"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.schema"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Chaque colonne d'un Spark DataFrame est modélisée comme un objet StructField avec un nom, un columnType et des propriétés \"nullables\". L'ensemble du schéma de la DataFrame est modélisé comme un `StructType`, qui est une collection d'objets `StructField`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Créons un schéma pour un DataFrame qui comporte des colonnes `first_name` et `age`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.sql.types._\n",
       "\n",
       "\u001b[39m\r\n",
       "\u001b[36mres15_1\u001b[39m: \u001b[32mStructType\u001b[39m = \u001b[33mStructType\u001b[39m(\n",
       "  \u001b[33mStructField\u001b[39m(\u001b[32m\"first_name\"\u001b[39m, StringType, true, {}),\n",
       "  \u001b[33mStructField\u001b[39m(\u001b[32m\"age\"\u001b[39m, DoubleType, true, {})\n",
       ")"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.types._\n",
    "\n",
    "StructType(\n",
    "    Seq(\n",
    "        StructField(\"first_name\", StringType, true),\n",
    "        StructField(\"age\", DoubleType, true)\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "L'interface de programmation de Spark permet de définir facilement le schéma exact que vous souhaitez pour vos DataFrames."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Créer des DataFrames avec createDataFrame()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La méthode `toDF()` pour créer des DataFrames est rapide, mais elle est limitée car elle ne vous permet pas de définir votre schéma (elle déduit le schéma pour vous). La méthode createDataFrame() vous permet de définir le schéma de votre DataFrame:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.sql.types._ \n",
       "\u001b[39m\r\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.sql.Row\n",
       "\n",
       "\u001b[39m\r\n",
       "\u001b[36manimalData\u001b[39m: \u001b[32mSeq\u001b[39m[\u001b[32mRow\u001b[39m] = \u001b[33mList\u001b[39m([30,bat], [2,mouse], [25,horse])\r\n",
       "\u001b[36manimalSchema\u001b[39m: \u001b[32mList\u001b[39m[\u001b[32mStructField\u001b[39m] = \u001b[33mList\u001b[39m(\n",
       "  \u001b[33mStructField\u001b[39m(\u001b[32m\"average_lifespan\"\u001b[39m, IntegerType, true, {}),\n",
       "  \u001b[33mStructField\u001b[39m(\u001b[32m\"animal_type\"\u001b[39m, StringType, true, {})\n",
       ")\r\n",
       "\u001b[36manimalDF\u001b[39m: \u001b[32mDataFrame\u001b[39m = [average_lifespan: int, animal_type: string]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.types._ \n",
    "import org.apache.spark.sql.Row\n",
    "\n",
    "val animalData = Seq(\n",
    "    Row(30, \"bat\"),\n",
    "    Row(2, \"mouse\"),\n",
    "    Row(25, \"horse\")\n",
    ")\n",
    "\n",
    "val animalSchema = List(\n",
    "    StructField(\"average_lifespan\", IntegerType, true), \n",
    "    StructField(\"animal_type\", StringType, true)\n",
    ")\n",
    "\n",
    "val animalDF = spark.createDataFrame(\n",
    "    spark.sparkContext.parallelize(animalData), \n",
    "    StructType(animalSchema)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<script>\n",
       "var comm = Jupyter.notebook.kernel.comm_manager.new_comm('cancel-stage-98155742-66e7-4391-9488-af8467001d54', {});\n",
       "\n",
       "function cancelStage(stageId) {\n",
       "  console.log('Cancelling stage ' + stageId);\n",
       "  comm.send({ 'stageId': stageId });\n",
       "}\n",
       "</script>\n",
       "          "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "  <span style=\"float: left;\">show at cmd17.sc:1</span>\n",
       "</div>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div class=\"progress\">\n",
       "  <div class=\"progress-bar\" role=\"progressbar\" style=\"background-color: blue; width: 100%; word-wrap: normal; white-space: nowrap; text-align: center; color: white\" aria-valuenow=\"100\" aria-valuemin=\"0\" aria-valuemax=\"100\">\n",
       "    1 / 1\n",
       "  </div>\n",
       "  <div class=\"progress-bar\" role=\"progressbar\" style=\"background-color: red; width: 0%\" aria-valuenow=\"0\" aria-valuemin=\"0\" aria-valuemax=\"100\"></div>\n",
       "</div>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "  <span style=\"float: left;\">show at cmd17.sc:1</span>\n",
       "</div>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div class=\"progress\">\n",
       "  <div class=\"progress-bar\" role=\"progressbar\" style=\"background-color: blue; width: 100%; word-wrap: normal; white-space: nowrap; text-align: center; color: white\" aria-valuenow=\"100\" aria-valuemin=\"0\" aria-valuemax=\"100\">\n",
       "    3 / 3\n",
       "  </div>\n",
       "  <div class=\"progress-bar\" role=\"progressbar\" style=\"background-color: red; width: 0%\" aria-valuenow=\"0\" aria-valuemin=\"0\" aria-valuemax=\"100\"></div>\n",
       "</div>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+-----------+\n",
      "|average_lifespan|animal_type|\n",
      "+----------------+-----------+\n",
      "|              30|        bat|\n",
      "|               2|      mouse|\n",
      "|              25|      horse|\n",
      "+----------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "animalDF.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous pouvons utiliser la méthode `animalDF.printSchema()` pour confirmer que le schéma a été créé comme spécifié:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- average_lifespan: integer (nullable = true)\n",
      " |-- animal_type: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "animalDF.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Les DataFrames sont les éléments fondamentaux de Spark. Toutes les analyses d'apprentissage machine et de streaming sont construites sur l'API DataFrame."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Voyons maintenant comment construire des fonctions pour manipuler les DataFrames."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Travailler avec des fichiers CSV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Les fichiers CSV sont parfaits pour apprendre Spark.\n",
    "Lorsque vous construisez de gros systèmes de données, vous voudrez généralement utiliser un format de fichier plus sophistiqué comme Parquet ou Avro, mais nous utiliserons généralement les CSV dans ce cours car ils sont lisibles par l'homme.\n",
    "Une fois que vous avez appris à utiliser les fichiers CSV, il est facile d'utiliser d'autres formats de fichiers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lecture d'un fichier CSV dans un DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Créons un fichier CSV avec ce chemin : `data/cat_data/inputs/file1.csv`. Le fichier doit contenir ces données :"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "cat_name,cat_age\n",
    "fluffy,4\n",
    "spot,3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "  <span style=\"float: left;\">csv at cmd19.sc:5</span>\n",
       "</div>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div class=\"progress\">\n",
       "  <div class=\"progress-bar\" role=\"progressbar\" style=\"background-color: blue; width: 100%; word-wrap: normal; white-space: nowrap; text-align: center; color: white\" aria-valuenow=\"100\" aria-valuemin=\"0\" aria-valuemax=\"100\">\n",
       "    1 / 1\n",
       "  </div>\n",
       "  <div class=\"progress-bar\" role=\"progressbar\" style=\"background-color: red; width: 0%\" aria-valuenow=\"0\" aria-valuemin=\"0\" aria-valuemax=\"100\"></div>\n",
       "</div>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\u001b[36mpath\u001b[39m: \u001b[32mString\u001b[39m = \u001b[32m\"data/cat_data/inputs/file1.csv\"\u001b[39m\r\n",
       "\u001b[36mdf\u001b[39m: \u001b[32mDataFrame\u001b[39m = [cat_name: string, cat_age: string]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val path = \"data/cat_data/inputs/file1.csv\"\n",
    "val df = spark\n",
    "        .read\n",
    "        .option(\"header\",\"true\")\n",
    "        .csv(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Affichons le contenu du DataFrame :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "  <span style=\"float: left;\">show at cmd20.sc:1</span>\n",
       "</div>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div class=\"progress\">\n",
       "  <div class=\"progress-bar\" role=\"progressbar\" style=\"background-color: blue; width: 100%; word-wrap: normal; white-space: nowrap; text-align: center; color: white\" aria-valuenow=\"100\" aria-valuemin=\"0\" aria-valuemax=\"100\">\n",
       "    1 / 1\n",
       "  </div>\n",
       "  <div class=\"progress-bar\" role=\"progressbar\" style=\"background-color: red; width: 0%\" aria-valuenow=\"0\" aria-valuemin=\"0\" aria-valuemax=\"100\"></div>\n",
       "</div>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------+\n",
      "|cat_name|cat_age|\n",
      "+--------+-------+\n",
      "|  fluffy|      4|\n",
      "|    spot|      3|\n",
      "+--------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Examinons également le schema du DataFrame :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- cat_name: string (nullable = true)\n",
      " |-- cat_age: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spark en déduit que les colonnes sont des chaînes de caractères.\n",
    "Vous pouvez également définir manuellement le schéma d'un CSV lors de son chargement dans un DataFrame.\n",
    "Dans la suite du cours, nous expliquerons comment demander à Spark de charger la colonne cat_age en tant qu'entier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ecriture d'un DataFrame sur disque"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ajoutons une colonne `speak` au DataFrame et écrivons les données sur le disque:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import org.apache.spark.sql.functions.lit\n",
    "\n",
    "df\n",
    "  .withColumn(\"speak\", lit(\"meow\"))\n",
    "  .write\n",
    "  .mode(\"overwrite\")\n",
    "  .csv(\"data/cat_data/outputs/cat_output1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Les méthodes de colonnes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La classe Spark Column définit une variété de méthodes de colonnes pour manipuler les DataFrames.\n",
    "Cette section montre comment instancier les objets Column et comment utiliser les plus importantes méthodes de colonnes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Un exemple simple "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Créons un DataFrame avec les superhéros et leur ville d'origine:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mdf\u001b[39m: \u001b[32mDataFrame\u001b[39m = [superhero: string, city: string]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val df = Seq(\n",
    "    (\"thor\", \"new york\"), \n",
    "    (\"aquaman\", \"atlantis\"), \n",
    "    (\"wolverine\", \"new york\")\n",
    ").toDF(\"superhero\",\"city\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Utilisons la méthode `startsWith()` pour identifier toutes les villes qui commencent par le mot `new` :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------+--------------------+\n",
      "|superhero|    city|city_starts_with_new|\n",
      "+---------+--------+--------------------+\n",
      "|     thor|new york|                true|\n",
      "|  aquaman|atlantis|               false|\n",
      "|wolverine|new york|                true|\n",
      "+---------+--------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df\n",
    "  .withColumn(\"city_starts_with_new\", $\"city\".startsWith(\"new\"))\n",
    "  .show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La partie `$\"city\"` du code crée un objet Colonne."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Examinons les différentes façons de créer des objets Column."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instanciation des objets colonnes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Les objets colonnes doivent être créés pour exécuter les méthodes de colonne.\n",
    "Un objet Colonne correspondant à la colonne city peut être créé en utilisant les trois syntaxes suivantes :"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "1. $\"city\"\n",
    "2. df(\"city\")\n",
    "3. col(\"city\") (must run import org.apache.spark.sql.functions.col first)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Les objets colonnes sont généralement passés en argument aux fonctions SQL (par exemple `upper($\"city\"))`. Nous allons créer des objets colonnes dans tous les exemples qui suivent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### gt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Créons un DataFrame avec une colonne de nombres entiers afin de pouvoir utiliser des méthodes de colonnes numériques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mdf\u001b[39m: \u001b[32mDataFrame\u001b[39m = [num: int, word: string]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val df = Seq(\n",
    "    (10, \"cat\"), \n",
    "    (4, \"dog\"), \n",
    "    (7, null)\n",
    ").toDF(\"num\",\"word\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+\n",
      "|num|word|\n",
      "+---+----+\n",
      "| 10| cat|\n",
      "|  4| dog|\n",
      "|  7|null|\n",
      "+---+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Utilisons la méthode `gt()` (greater than) pour identifier toutes les lignes ayant un nombre supérieur à cinq:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+--------+\n",
      "|num|word|num_gt_5|\n",
      "+---+----+--------+\n",
      "| 10| cat|    true|\n",
      "|  4| dog|   false|\n",
      "|  7|null|    true|\n",
      "+---+----+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df\n",
    "  .withColumn(\"num_gt_5\", col(\"num\").gt(5))\n",
    "  .show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous pouvons également utiliser l'opérateur `>` pour effectuer des comparaisons \"supérieures à\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+--------+\n",
      "|num|word|num_gt_5|\n",
      "+---+----+--------+\n",
      "| 10| cat|    true|\n",
      "|  4| dog|   false|\n",
      "|  7|null|    true|\n",
      "+---+----+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df\n",
    "  .withColumn(\"num_gt_5\", col(\"num\") >= 7)\n",
    "  .show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### substr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Utilisons la méthode `substr()` pour créer une nouvelle colonne avec les deux premières lettres du mot colonne:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+--------------+\n",
      "|num|word|word_first_two|\n",
      "+---+----+--------------+\n",
      "| 10| cat|            ca|\n",
      "|  4| dog|            do|\n",
      "|  7|null|          null|\n",
      "+---+----+--------------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.sql.functions.col\n",
       "\u001b[39m"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.functions.col\n",
    "df\n",
    "  .withColumn(\"word_first_two\", col(\"word\").substr(0, 2))\n",
    "  .show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notez que la méthode `substr()` renvoie `null` lorsqu'elle recoit null en entrée. Toutes les autres méthodes et fonctions SQL se comportent de la même manière (c'est-à-dire qu'elles renvoient `null` lorsque l'entrée est `null`)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vos fonctions doivent traiter les entrées nulles avec élégance et renvoyer `null` lorsqu'elles recoivent `null` en entrée."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### L'operatuer `+`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Utilisons l'opérateur `+` pour ajouter cinq à la colonne des nombres:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+-------------+\n",
      "|num|word|num_plus_five|\n",
      "+---+----+-------------+\n",
      "| 10| cat|           15|\n",
      "|  4| dog|            9|\n",
      "|  7|null|           12|\n",
      "+---+----+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df\n",
    "  .withColumn(\"num_plus_five\", col(\"num\").+(5))\n",
    "  .show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous pouvons également sauter la notation par points lorsque nous invoquons la fonction:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+-------------+\n",
      "|num|word|num_plus_five|\n",
      "+---+----+-------------+\n",
      "| 10| cat|           15|\n",
      "|  4| dog|            9|\n",
      "|  7|null|           12|\n",
      "+---+----+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df\n",
    "  .withColumn(\"num_plus_five\", col(\"num\") + 5)\n",
    "  .show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le *syntactic sugar* rend plus difficile de voir que `+` est une méthode définie dans la classe Colonne."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### lit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Utilisons la méthode `/` pour prendre `2` divisé par la colonne des `num`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+------------------+\n",
      "|num|word|two_divided_by_num|\n",
      "+---+----+------------------+\n",
      "| 10| cat|               0.2|\n",
      "|  4| dog|               0.5|\n",
      "|  7|null|0.2857142857142857|\n",
      "+---+----+------------------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.sql.functions._\n",
       "\u001b[39m"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.functions._\n",
    "df\n",
    "  .withColumn(\"two_divided_by_num\", lit(2) / col(\"num\"))\n",
    "  .show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notez que la fonction `lit()` doit être utilisée pour convertir 2 en un objet Column avant que la division puisse avoir lieu."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "cmd28.sc:3: overloaded method value / with alternatives:\n",
      "  (x: Double)Double <and>\n",
      "  (x: Float)Float <and>\n",
      "  (x: Long)Long <and>\n",
      "  (x: Int)Int <and>\n",
      "  (x: Char)Int <and>\n",
      "  (x: Short)Int <and>\n",
      "  (x: Byte)Int\n",
      " cannot be applied to (org.apache.spark.sql.Column)\n",
      "  .withColumn(\"two_divided_by_num\", 2 / col(\"num\"))\n",
      "                                      ^Compilation Failed"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "Compilation Failed"
     ]
    }
   ],
   "source": [
    "df\n",
    "  .withColumn(\"two_divided_by_num\", 2 / col(\"num\"))\n",
    "  .show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La méthode `/` est définie dans les deux classes Scala Int et Spark Column. Nous devons convertir le nombre en un objet Column, afin que le compilateur sache qu'il doit utiliser la méthode `/` définie dans la classe Spark Column. En analysant le message d'erreur, nous pouvons voir que le compilateur essaie par erreur d'utiliser l'opérateur `/` défini dans la classe Scala Int."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### isNull"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Utilisons la méthode `isNull` pour identifier quand la colonne word est `null`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+------------+\n",
      "|num|word|word_is_null|\n",
      "+---+----+------------+\n",
      "| 10| cat|       false|\n",
      "|  4| dog|       false|\n",
      "|  7|null|        true|\n",
      "+---+----+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df\n",
    "  .withColumn(\"word_is_null\", col(\"word\").isNull)\n",
    "  .show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### when / otherwise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Créons un DataFrame final avec les colonnes `word1` et `word2`, afin de pouvoir jouer avec les méthodes `===`, `when()`, et `otherwise()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mdf\u001b[39m: \u001b[32mDataFrame\u001b[39m = [word1: string, word2: string]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val df = Seq( \n",
    "    (\"bat\", \"bat\"), \n",
    "    (\"snake\", \"rat\"), \n",
    "    (\"cup\", \"phone\"), \n",
    "    (\"key\", null)\n",
    ").toDF(\"word1\",\"word2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+\n",
      "|word1|word2|\n",
      "+-----+-----+\n",
      "|  bat|  bat|\n",
      "|snake|  rat|\n",
      "|  cup|phone|\n",
      "|  key| null|\n",
      "+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ecrivons un petit algorithme de comparaison de mots qui analyse les différences entre les deux mots:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+---------------+\n",
      "|word1|word2|word_comparison|\n",
      "+-----+-----+---------------+\n",
      "|  bat|  bat|     same words|\n",
      "|snake|  rat|word1 is longer|\n",
      "|  cup|phone|  i am confused|\n",
      "|  key| null|  i am confused|\n",
      "+-----+-----+---------------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.sql.functions._\n",
       "\u001b[39m"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.functions._\n",
    "df\n",
    "  .withColumn(\n",
    "    \"word_comparison\",\n",
    "    when($\"word1\" === $\"word2\", \"same words\")\n",
    "      .when(length($\"word1\") > length($\"word2\"), \"word1 is longer\")\n",
    "      .otherwise(\"i am confused\")\n",
    "  ).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`when()` et `otherwise()` sont la façon d'écrire la logique `if / else if / else` dans Spark."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vous utiliserez tout le temps des méthode colonnes lorsque vous écrirez du code Spark.\n",
    "Si vous n'avez pas de solides connaissances en programmation orientée objet, il peut être difficile de déterminer quelles méthodes sont définies dans la classe `Column` et quelles méthodes sont définies dans le paquet `org.apache.spark.sql.functions`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction aux fonctions de Spark SQL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cette section vous montre comment utiliser les fonctions SQL de Spark et comment construire vos propres fonctions SQL. Les fonctions Spark SQL sont essentielles pour presque toutes les analyses."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Les fonctions Spark SQL sont définies dans l'objet `org.apache.spark.sql.functions`. Il y a une tonne de fonctions ! (cf. Spark Doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La plupart des fonctions SQL prennent un ou des  argument(s) de type Column et renvoient des objets Column."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Montrons comment utiliser une fonction SQL. \n",
    "Créons un DataFrame avec une colonne `number` et utilisons la fonction `factorial` pour ajouter une colonne `number_factorial`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.sql.functions._\n",
    "\n",
    "val df = Seq(2,3,4).toDF(\"number\")\n",
    "df\n",
    " .withColumn(\"number_factorial\", factorial(col(\"number\")))\n",
    " .show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si `Spark implicits` est importés (c'est-à-dire que vous avez lancé l'importation `spark.implicits._`), vous pouvez également créer un objet Column avec l'opérateur `$`. Ce code fonctionne également:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.sql.functions._\n",
    "import spark.implicits._\n",
    "\n",
    "val df = Seq(2,3,4).toDF(\"number\")\n",
    "\n",
    "df\n",
    " .withColumn(\"number_factorial\", factorial($\"number\"))\n",
    " .show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le reste de cette section se concentre sur les fonctions SQL les plus importantes qui seront utilisées dans la plupart des analyses."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### lit() function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La fonction `lit()` crée un objet Column à partir d'une valeur littérale. Créons un DataFrame et utilisons la fonction `lit()` pour ajouter une colonne `wolof_hi` au DataFrame:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.sql.functions.lit\n",
    "\n",
    "val df = Seq(\"madeleine\",\"anta\",\"zakaria\").toDF(\"word\") \n",
    "df\n",
    " .withColumn(\"wolof_hi\", lit(\"ziyaar\"))\n",
    " .show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La fonction `lit()` est particulièrement utile pour faire des comparaisons booléennes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### when() et otherwise()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Les fonctions `when()` et `otherwise()` sont utilisées pour le flux de contrôle dans Spark SQL, de la même manière que if et else dans d'autres langages de programmation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Créons un DataFrame `contries` et utilisons des instructions `when()` pour ajouter une colonne  `continent`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val df = Seq(\"senegal\",\"canada\",\"italy\",\"tralfamadore\").toDF(\"country\")\n",
    "df\n",
    "  .withColumn(\n",
    "    \"continent\",\n",
    "    when(col(\"country\") === lit(\"senegal\"), lit(\"africa\"))\n",
    "      .when(col(\"country\") === lit(\"canada\"), lit(\"north america\"))\n",
    "      .when(col(\"country\") === lit(\"italy\"), lit(\"europe\"))\n",
    "      .otherwise(\"not sure\")\n",
    ") .show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spark vous permet de couper parfois les appels à la méthode lit() et d'exprimer le code de manière compacte:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df\n",
    "  .withColumn(\n",
    "    \"continent\",\n",
    "    when(col(\"country\") === \"senegal\", \"africa\")\n",
    "      .when(col(\"country\") === \"canada\", \"north america\")\n",
    "      .when(col(\"country\") === \"italy\", \"europe\")\n",
    "      .otherwise(\"not sure\")\n",
    ") .show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mdf\u001b[39m: \u001b[32mDataFrame\u001b[39m = [Prenom: string, Nom: string]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val df = Seq( \n",
    "    (\"Amina\", \"Ba\"), \n",
    "    (\"Modou\", \"Ndiaye\"), \n",
    "    (\"Luiz\", \"Faye\"), \n",
    ").toDF(\"Prenom\",\"Nom\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------+------------+\n",
      "|Prenom|   Nom|    fullName|\n",
      "+------+------+------------+\n",
      "| Amina|    Ba|    Amina Ba|\n",
      "| Modou|Ndiaye|Modou Ndiaye|\n",
      "|  Luiz|  Faye|   Luiz Faye|\n",
      "+------+------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df\n",
    "    .withColumn(\"fullName\", concat(col(\"Prenom\"),col(\"Nom\")))\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La méthode when est définie à la fois dans la classe Column et l'objet functions. Chaque fois que vous voyez `when()` qui n'est pas précédé d'un point, c'est alors when de l'objet functions. `.when()` vient de la classe Column."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rédiger sa propre fonction SQL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vous pouvez facilement créer vos propres fonctions SQL. Beaucoup de nouveaux développeurs Spark créent des fonctions définies par l'utilisateur alors qu'il serait beaucoup plus facile de créer simplement une fonction SQL personnalisée. Évitez les fonctions définies par l'utilisateur dans la mesure du possible !"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Créons une fonction `lifeStage()` qui prend un argument `age` et renvoie \"child\", \"teenager\" ou \"adult\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.sql.Column\n",
    "\n",
    "def lifeStage(col: Column): Column = {\n",
    "    when(col < 13, \"child\")\n",
    "    .when(col >= 13 && col <= 18, \"teenager\")\n",
    "    .when(col > 18, \"adult\")\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Voici comment utiliser la fonction lifeStage() :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val df = Seq(10,15,25).toDF(\"age\")\n",
    "\n",
    "df\n",
    "  .withColumn(\n",
    "      \"life_stage\",\n",
    "      lifeStage(col(\"age\"))\n",
    "  )\n",
    "  .show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.sql.Column\n",
    "import org.apache.spark.sql.functions.col\n",
    "\n",
    "def lifeStageString(colName: String): Column = {\n",
    "    lifeStage(col(colName))\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val df = Seq(10,15,25).toDF(\"age\")\n",
    "\n",
    "df\n",
    "  .withColumn(\n",
    "      \"life_stage\",\n",
    "      lifeStageString(\"age\")\n",
    "  )\n",
    "  .show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Créons une autre fonction qui enleve tous les espaces et met en majuscules tous les caractères d'une chaîne:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.sql.Column\n",
    "\n",
    "def trimUpper(col: Column): Column={\n",
    "    trim(upper(col))\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lançons trimUpper() sur un échantillon de données:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val df = Seq(\n",
    "    \"   some stuff\",\n",
    "    \"like CHEESE \",\n",
    "    \"null\"\n",
    ").toDF(\"weird\")\n",
    "\n",
    "df\n",
    "  .withColumn(\n",
    "      \"cleaned\",\n",
    "      trimUpper(col(\"weird\"))\n",
    "  )\n",
    "  .show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Des fonctions SQL personnalisées peuvent généralement être utilisées à la place des UDFs. Éviter les UDFs est un excellent moyen d'écrire un meilleur code Spark."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Les fonctions Spark SQL sont préférables aux UDFs parce qu'elles gèrent la cas null de maniere elegante (sans beaucoup de code) et parce qu'elles ne sont pas une boîte noire.\n",
    "\n",
    "La plupart des analyses Spark peuvent être exécutées en utilisant la bibliothèque standard et en revenant à des fonctions SQL personnalisées si nécessaire. Évitez les UDFs à tout prix !"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Enchaînement de transformations de DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cette section explique comment écrire des transformations de DataFrame et comment enchaîner des transformations multiples avec la méthode `Dataset#transform`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### La méthode `transform` "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La méthode de `transform` de Dataset fournit une syntaxe concise pour l'enchaînement des transformations personnalisées."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Supposons que nous ayons une méthode `withGreeting()` qui ajoute une colonne `greeting` à un DataFrame et un `withFarewell()` methode qui ajoute une colonne `farewell` à un DataFrame:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.sql.DataFrame\n",
    "import org.apache.spark.sql.functions.lit\n",
    "\n",
    "def withGreeting(df: DataFrame): DataFrame = {\n",
    "    df.withColumn(\"greeting\", lit(\"hello world\"))\n",
    "}\n",
    "\n",
    "def withFarewell(df: DataFrame): DataFrame = {\n",
    "    df.withColumn(\"farewell\", lit(\"goodbye\"))\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val df = Seq( \n",
    "    \"Aby\", \"Mohamed\"\n",
    ").toDF(\"someone\")\n",
    "\n",
    "\n",
    "val weirdDF = df\n",
    " .transform(withGreeting)\n",
    " .transform(withFarewell)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weirdDF.show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La méthode transform peut facilement être chaînée avec les méthodes intégrées de Spark DataFrame, comme select:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df\n",
    "  .select(\"someone\")\n",
    "  .transform(withGreeting)\n",
    "  .transform(withFarewell)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La méthode transform nous aide à écrire du code facile à suivre en évitant les appels de méthodes imbriquées. Sans la méthode transform, le code ci-dessus devient moins lisible :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "withFarewell(withGreeting(df)).show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ou pire encore:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "withFarewell(withGreeting(df)).select(\"someone\").show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### La méthode `transform` avec arguments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nos deux exemples de transformations precedentes (withFarewell et withGreeting) modifient les DataFrames de manière standard : c'est-à-dire qu'ils ajouteront toujours une colonne nommée farewell et greeting, chacune avec des valeurs codées en dur (\"goodbye\" et \"hello world\", respectivement)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous pouvons également créer des transformations de DataFrame personnalisées en définissant des transformations qui prennent des arguments. Pour cela, nous pouvons exploiter le currying avec des listes de paramètres multiples dans Scala.\n",
    "\n",
    "Pour illustrer la différence, utilisons la même méthode `withGreeting()` que précédemment et ajoutons une méthode `withCat()` qui prend une chaîne de caractères comme argument:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def withGreeting(df: DataFrame): DataFrame = {\n",
    "    df.withColumn(\"greeting\", lit(\"hello world\"))\n",
    "}\n",
    "\n",
    "def withCat(name: String)(df: DataFrame): DataFrame = {\n",
    "    df.withColumn(\"cats\", lit(s\"$name meow\"))\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous pouvons utiliser la méthode `transform` pour exécuter les méthodes `withGreeting()` et `withCat()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val df = Seq( \n",
    "    \"funny\", \n",
    "    \"person\"\n",
    ").toDF(\"something\")\n",
    "\n",
    "val niceDF = df\n",
    "  .transform(withGreeting) \n",
    "  .transform(withCat(\"puffy\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "niceDF.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Effectuer des opérations sur plusieurs colonnes avec foldLeft"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La méthode `foldLeft` de Scala peut être utilisée pour itérer sur une structure de données et effectuer de multiples opérations sur un Spark DataFrame.\n",
    "\n",
    "Par exemple, foldLeft peut être utilisé pour éliminer tous les espaces dans plusieurs colonnes ou pour convertir tous les noms de colonnes d'un DataFrame en snake_case.\n",
    "\n",
    "foldLeft est idéal lorsque vous souhaitez effectuer des opérations similaires sur plusieurs colonnes. Plongeons dans le vif du sujet !"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Revue de foldLeft sur Scala"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Supposons que vous ayez une liste de trois nombres impairs et que vous souhaitiez calculer la somme de tous les nombres de la liste."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La méthode foldLeft permet d'itérer sur chaque élément de la liste et de garder une trace d'une somme courante:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val odds = List(1,5,7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "println {\n",
    "    odds.foldLeft(0) {\n",
    "        (memo: Int, num: Int) => memo + num \n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "C'est la meme chose que la boucle suivante:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "var memo: Int = 0\n",
    "for (num <- odds)  {\n",
    "    memo += num // memo = memo + num\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La fonction foldLeft est initialisée avec une valeur de départ de zéro et la somme courante est accumulée dans la variable mémo. Ce code additionne tous les nombres de la liste."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Éliminer les espaces sur plusieurs colonnes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Créons un DataFrame et écrivons ensuite une fonction pour supprimer tous les espaces dans toutes les colonnes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val sourceDF = Seq(\n",
    "    (\" N e y m a r\", \"Brasil\"), (\"Sadio\", \"S e negal\")\n",
    ").toDF(\"name\",\"country\")\n",
    "\n",
    "sourceDF.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val actualDF = Seq( \n",
    "    \"name\",\n",
    "    \"country\"\n",
    ").foldLeft(sourceDF) {\n",
    "    (memoDF, colName) => memoDF.withColumn(\n",
    "        colName,\n",
    "        removeAllWhiteSpace(colName)\n",
    "  )\n",
    "}\n",
    "\n",
    "def removeAllWhiteSpace(colName: String): Column = {\n",
    "    regexp_replace(col(colName), \"\\\\s+\", \"\")\n",
    "}\n",
    "\n",
    "actualDF.show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction à la jointure  de diffusion avec Spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Les jointures de diffusion (broadcast joins) sont parfaites pour relier un grand DataFrame à un petit DataFrame. Les jointures de diffusion ne peuvent pas être utilisées pour joindre deux grands DataFrames.\n",
    "\n",
    "Cette section explique comment réaliser une simple jointure de diffusion et comment la fonction broadcast() aide Spark à optimiser le plan d'exécution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Concepte"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spark répartit les données sur les différents nœuds d'un cluster afin que plusieurs ordinateurs puissent traiter les données en parallèle. Les jointures traditionnelles sont difficiles avec Spark car les données sont réparties sur plusieurs machines.\n",
    "\n",
    "Les liaisons de diffusion sont plus faciles à exécuter sur un cluster. Spark peut \"diffuser\" une petite DataFrame en envoyant toutes les données de cette petite DataFrame à tous les nœuds du cluster. Une fois le petit DataFrame diffusée, Spark peut effectuer une jointure sans un shuffling des données du grand DataFrame."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exemple"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Créons un DataFrame avec des informations sur les personnes et un autre DataFrame avec des informations sur les villes. Dans cet exemple, les deux DataFrames seront petits, mais imaginons que le `peopleDF` est énorme et le `citiesDF` est minuscule:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val peopleDF = Seq( \n",
    "    (\"khadidiatou\", \"dakar\"), \n",
    "    (\"bally\", \"dakar\"), \n",
    "    (\"bayoulou\", \"bobo\")\n",
    ").toDF(\"first_name\",\"city\") \n",
    "\n",
    "peopleDF.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val citiesDF = Seq(\n",
    "    (\"dakar\", \"senegal\", 0.1),\n",
    "    (\"bobo\", \"burkina\", 0.25)\n",
    ").toDF(\"city\",\"country\",\"population\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La jointure simple des deux dataframes peut se faire en utilisant la methode `.join()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "peopleDF.join(\n",
    "    citiesDF,\n",
    "    peopleDF(\"city\") <=> citiesDF(\"city\"),\n",
    "    \"inner\"\n",
    ").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Diffusons maintenant citiesDF et joignons le avec peopleDF:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "peopleDF.join(\n",
    "    broadcast(citiesDF),\n",
    "    peopleDF(\"city\") <=> citiesDF(\"city\")\n",
    ").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "L'opérateur d'égalité Spark \"null safe\" (`<=>`) est utilisé pour effectuer cette jointure."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyse des plans physiques des jointures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Utilisons la méthode `explain()` pour analyser le plan physique de la jointure de diffusion:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "peopleDF.join(\n",
    "  broadcast(citiesDF),\n",
    "  peopleDF(\"city\") <=> citiesDF(\"city\")\n",
    ").explain()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dans cet exemple, Spark est assez intelligent pour renvoyer le même plan physique, même lorsque la méthode `broadcast()` n'est pas utilisée:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "peopleDF.join(\n",
    "  citiesDF,\n",
    "  peopleDF(\"city\") <=> citiesDF(\"city\")\n",
    ").explain()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Élimination de la colonne `city` en double"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous pouvons passer une séquence de colonnes en argument pour supprimer automatiquement la colonne en double:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "peopleDF.join( \n",
    "    broadcast(citiesDF), \n",
    "    Seq(\"city\")\n",
    ").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Examinons le plan physique généré par ce code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "peopleDF.join( \n",
    "    broadcast(citiesDF), \n",
    "    q(\"city\")Se\n",
    ").explain()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Un code qui renvoie le même résultat sans s'appuyer sur la séquence jointe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "peopleDF.join(\n",
    "    broadcast(citiesDF),\n",
    "    peopleDF(\"city\") <=> citiesDF(\"city\")\n",
    ")\n",
    ".drop(citiesDF(\"city\"))\n",
    ".explain()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Il est préférable d'éviter le raccourci \"join syntax\" pour que vos plans physiques restent aussi simples que possible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vous pouvez passer a la méthode `explain()` un argument `true` pour voir le plan logique parsé, le plan logique analysé et le plan logique optimisé en plus du plan physique:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "peopleDF.join(\n",
    "  broadcast(citiesDF),\n",
    "  peopleDF(\"city\") <=> citiesDF(\"city\")\n",
    ")\n",
    ".drop(citiesDF(\"city\"))\n",
    ".explain(true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remarquez comment les plans logiques parsés, analysés et optimisés contiennent tous `ResolvedHint (broadcast)` car la fonction broadcast() a été utilisée. Cet indice n'est pas inclus lorsque la fonction broadcast() n'est pas utilisée:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "peopleDF.join(\n",
    "  citiesDF,\n",
    "  peopleDF(\"city\") <=> citiesDF(\"city\")\n",
    ")\n",
    ".drop(citiesDF(\"city\"))\n",
    ".explain(true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Les jointures de diffusion sont un excellent moyen d'ajouter aux grands DataFrames des données stockées dans des fichiers de données de vérité relativement petits et provenant d'une seule source. Des DataFrames allant jusqu'à 2 Go peuvent être diffusés."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Scala",
   "language": "scala",
   "name": "scala"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".sc",
   "mimetype": "text/x-scala",
   "name": "scala",
   "nbconvert_exporter": "script",
   "version": "2.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
