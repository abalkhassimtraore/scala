{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Big Data avec Spark : Spark SQL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*`Nom & Prenom : `*\n",
    "Abal Khassim TRAORE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problematique"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ce projet consiste à utiliser Apache Spark pour faire l'analyse et le traitement des données de **[San Francisco Fire Department Calls ](https://data.sfgov.org/Public-Safety/Fire-Department-Calls-for-Service/nuek-vuh3)** afin de fournir quelques KPI (*Key Performance Indicator*). Le **SF Fire Datasets** comprend les réponses aux appels de toutes les unités d'incendie. Chaque enregistrement comprend le numéro d'appel, le numéro d'incident, l'adresse, l'identifiant de l'unité, le type d'appel et la disposition. Tous les intervalles de temps pertinents sont également inclus. Étant donné que ce Dataset est basé sur les réponses et que la plupart des appels impliquent plusieurs unités, ainsi il existe plusieurs enregistrements pour chaque numéro d'appel. Les adresses sont associées à un numéro de bloc, à une intersection ou à une boîte d'appel, et non à une adresse spécifique.\n",
    "\n",
    "**Plus de details sur la description des données [ici](https://data.sfgov.org/Public-Safety/Fire-Department-Calls-for-Service/nuek-vuh3)**\n",
    "\n",
    "**Download csv file [here](https://data.sfgov.org/api/views/nuek-vuh3/rows.csv?accessType=DOWNLOAD)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Travail à faire.\n",
    "L'objectif de ce travail est de comprendre le Dataset SF Fire afin de bien répondre aux questions en utilisant les codes Spark/Scala adéquats.\n",
    "\n",
    "- Code lisible et bien indenté, \n",
    "- N'oublier pas de mettre en commentaire la justification de votre réponse sur les cellule Markdown. \n",
    "\n",
    "\n",
    "#### Note:\n",
    "- Vous pouvez en groupe (au plus deux étudiants) . "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[39m\u001b[36m$ivy.$                                  \n",
       "\u001b[39m\r\n",
       "\u001b[32mimport \u001b[39m\u001b[36m$ivy.$                               \u001b[39m"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//creation d un dataframe\n",
    "import $ivy.`org.apache.spark::spark-sql:2.4.5`\n",
    "import $ivy.`sh.almond::almond-spark:0.10.9`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q1. Importez les modules Spark necessaires"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.log4j.{Level, Logger}\n",
       "\n",
       "\u001b[39m\r\n",
       "\u001b[36mrootLogger\u001b[39m: \u001b[32mLogger\u001b[39m = org.apache.log4j.spi.RootLogger@328949f6\r\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.sql.functions._\u001b[39m"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.log4j.{Level, Logger}\n",
    "\n",
    "val rootLogger = Logger.getRootLogger()\n",
    "rootLogger.setLevel(Level.ERROR)\n",
    "\n",
    "Logger.getLogger(\"org.apache.spark\").setLevel(Level.WARN)\n",
    "Logger.getLogger(\"org.spark-project\").setLevel(Level.WARN)\n",
    "import org.apache.spark.sql.functions._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q2. Creez la Spark Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/04/25 19:04:26 WARN SparkSession$Builder: Using an existing SparkSession; some configuration may not take effect.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.sql._\n",
       "\n",
       "\u001b[39m\r\n",
       "\u001b[36mspark\u001b[39m: \u001b[32mSparkSession\u001b[39m = org.apache.spark.sql.SparkSession@7d3274ad"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql._\n",
    "\n",
    "val spark = {\n",
    "  SparkSession.builder()\n",
    "    .master(\"local\")\n",
    "    .appName(\"BD-FS FIRE\")\n",
    "    .getOrCreate()\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q3. Chargez les données"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Utilisez le `fireSchema` definit dans la cellule suivante pour le chargement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.sql.types._\n",
       "\u001b[39m\r\n",
       "\u001b[36mfireSchema\u001b[39m: \u001b[32mStructType\u001b[39m = \u001b[33mStructType\u001b[39m(\n",
       "  \u001b[33mStructField\u001b[39m(\u001b[32m\"CallNumber\"\u001b[39m, IntegerType, true, {}),\n",
       "  \u001b[33mStructField\u001b[39m(\u001b[32m\"UnitID\"\u001b[39m, StringType, true, {}),\n",
       "  \u001b[33mStructField\u001b[39m(\u001b[32m\"IncidentNumber\"\u001b[39m, IntegerType, true, {}),\n",
       "  \u001b[33mStructField\u001b[39m(\u001b[32m\"CallType\"\u001b[39m, StringType, true, {}),\n",
       "  \u001b[33mStructField\u001b[39m(\u001b[32m\"CallDate\"\u001b[39m, StringType, true, {}),\n",
       "  \u001b[33mStructField\u001b[39m(\u001b[32m\"WatchDate\"\u001b[39m, StringType, true, {}),\n",
       "  \u001b[33mStructField\u001b[39m(\u001b[32m\"CallFinalDisposition\"\u001b[39m, StringType, true, {}),\n",
       "  \u001b[33mStructField\u001b[39m(\u001b[32m\"AvailableDtTm\"\u001b[39m, StringType, true, {}),\n",
       "  \u001b[33mStructField\u001b[39m(\u001b[32m\"Address\"\u001b[39m, StringType, true, {}),\n",
       "  \u001b[33mStructField\u001b[39m(\u001b[32m\"City\"\u001b[39m, StringType, true, {}),\n",
       "  \u001b[33mStructField\u001b[39m(\u001b[32m\"Zipcode\"\u001b[39m, IntegerType, true, {}),\n",
       "  \u001b[33mStructField\u001b[39m(\u001b[32m\"Battalion\"\u001b[39m, StringType, true, {}),\n",
       "  \u001b[33mStructField\u001b[39m(\u001b[32m\"StationArea\"\u001b[39m, StringType, true, {}),\n",
       "  \u001b[33mStructField\u001b[39m(\u001b[32m\"Box\"\u001b[39m, StringType, true, {}),\n",
       "  \u001b[33mStructField\u001b[39m(\u001b[32m\"OriginalPriority\"\u001b[39m, StringType, true, {}),\n",
       "  \u001b[33mStructField\u001b[39m(\u001b[32m\"Priority\"\u001b[39m, StringType, true, {}),\n",
       "  \u001b[33mStructField\u001b[39m(\u001b[32m\"FinalPriority\"\u001b[39m, IntegerType, true, {}),\n",
       "  \u001b[33mStructField\u001b[39m(\u001b[32m\"ALSUnit\"\u001b[39m, BooleanType, true, {}),\n",
       "  \u001b[33mStructField\u001b[39m(\u001b[32m\"CallTypeGroup\"\u001b[39m, StringType, true, {}),\n",
       "  \u001b[33mStructField\u001b[39m(\u001b[32m\"NumAlarms\"\u001b[39m, IntegerType, true, {}),\n",
       "  \u001b[33mStructField\u001b[39m(\u001b[32m\"UnitType\"\u001b[39m, StringType, true, {}),\n",
       "  \u001b[33mStructField\u001b[39m(\u001b[32m\"UnitSequenceInCallDispatch\"\u001b[39m, IntegerType, true, {}),\n",
       "  \u001b[33mStructField\u001b[39m(\u001b[32m\"FirePreventionDistrict\"\u001b[39m, StringType, true, {}),\n",
       "  \u001b[33mStructField\u001b[39m(\u001b[32m\"SupervisorDistrict\"\u001b[39m, StringType, true, {}),\n",
       "  \u001b[33mStructField\u001b[39m(\u001b[32m\"Neighborhood\"\u001b[39m, StringType, true, {}),\n",
       "  \u001b[33mStructField\u001b[39m(\u001b[32m\"Location\"\u001b[39m, StringType, true, {}),\n",
       "  \u001b[33mStructField\u001b[39m(\u001b[32m\"RowID\"\u001b[39m, StringType, true, {}),\n",
       "  \u001b[33mStructField\u001b[39m(\u001b[32m\"Delay\"\u001b[39m, FloatType, true, {})\n",
       ")"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.types._\n",
    "val fireSchema = StructType(Array(StructField(\"CallNumber\", IntegerType, true),\n",
    "  StructField(\"UnitID\", StringType, true),\n",
    "  StructField(\"IncidentNumber\", IntegerType, true),\n",
    "  StructField(\"CallType\", StringType, true),                  \n",
    "  StructField(\"CallDate\", StringType, true),      \n",
    "  StructField(\"WatchDate\", StringType, true),\n",
    "  StructField(\"CallFinalDisposition\", StringType, true),\n",
    "  StructField(\"AvailableDtTm\", StringType, true),\n",
    "  StructField(\"Address\", StringType, true),       \n",
    "  StructField(\"City\", StringType, true),       \n",
    "  StructField(\"Zipcode\", IntegerType, true),       \n",
    "  StructField(\"Battalion\", StringType, true),                 \n",
    "  StructField(\"StationArea\", StringType, true),       \n",
    "  StructField(\"Box\", StringType, true),       \n",
    "  StructField(\"OriginalPriority\", StringType, true),       \n",
    "  StructField(\"Priority\", StringType, true),       \n",
    "  StructField(\"FinalPriority\", IntegerType, true),       \n",
    "  StructField(\"ALSUnit\", BooleanType, true),       \n",
    "  StructField(\"CallTypeGroup\", StringType, true),\n",
    "  StructField(\"NumAlarms\", IntegerType, true),\n",
    "  StructField(\"UnitType\", StringType, true),\n",
    "  StructField(\"UnitSequenceInCallDispatch\", IntegerType, true),\n",
    "  StructField(\"FirePreventionDistrict\", StringType, true),\n",
    "  StructField(\"SupervisorDistrict\", StringType, true),\n",
    "  StructField(\"Neighborhood\", StringType, true),\n",
    "  StructField(\"Location\", StringType, true),\n",
    "  StructField(\"RowID\", StringType, true),\n",
    "  StructField(\"Delay\", FloatType, true)))\n",
    "\n",
    "// your code here (hint spark session name is sparkSession Q2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "le nombre de ligne est 175296\n",
      "le nombre de colonne est 28\n",
      "+----------+------+--------------+----------------+----------+----------+--------------------+--------------------+--------------------+----+-------+---------+-----------+----+----------------+--------+-------------+-------+-------------+---------+--------------+--------------------------+----------------------+------------------+--------------------+--------------------+-------------+---------+\n",
      "|CallNumber|UnitID|IncidentNumber|        CallType|  CallDate| WatchDate|CallFinalDisposition|       AvailableDtTm|             Address|City|Zipcode|Battalion|StationArea| Box|OriginalPriority|Priority|FinalPriority|ALSUnit|CallTypeGroup|NumAlarms|      UnitType|UnitSequenceInCallDispatch|FirePreventionDistrict|SupervisorDistrict|        Neighborhood|            Location|        RowID|    Delay|\n",
      "+----------+------+--------------+----------------+----------+----------+--------------------+--------------------+--------------------+----+-------+---------+-----------+----+----------------+--------+-------------+-------+-------------+---------+--------------+--------------------------+----------------------+------------------+--------------------+--------------------+-------------+---------+\n",
      "|  20110016|   T13|       2003235|  Structure Fire|01/11/2002|01/10/2002|               Other|01/11/2002 01:51:...|2000 Block of CAL...|  SF|  94109|      B04|         38|3362|               3|       3|            3|  false|         null|        1|         TRUCK|                         2|                     4|                 5|     Pacific Heights|(37.7895840679362...|020110016-T13|     2.95|\n",
      "|  20110022|   M17|       2003241|Medical Incident|01/11/2002|01/10/2002|               Other|01/11/2002 03:01:...|0 Block of SILVER...|  SF|  94124|      B10|         42|6495|               3|       3|            3|   true|         null|        1|         MEDIC|                         1|                    10|                10|Bayview Hunters P...|(37.7337623673897...|020110022-M17|      4.7|\n",
      "|  20110023|   M41|       2003242|Medical Incident|01/11/2002|01/10/2002|               Other|01/11/2002 02:39:...|MARKET ST/MCALLIS...|  SF|  94102|      B03|         01|1455|               3|       3|            3|   true|         null|        1|         MEDIC|                         2|                     3|                 6|          Tenderloin|(37.7811772186856...|020110023-M41|2.4333334|\n",
      "|  20110032|   E11|       2003250|    Vehicle Fire|01/11/2002|01/10/2002|               Other|01/11/2002 04:16:...|APPLETON AV/MISSI...|  SF|  94110|      B06|         32|5626|               3|       3|            3|  false|         null|        1|        ENGINE|                         1|                     6|                 9|      Bernal Heights|(37.7388432849018...|020110032-E11|      1.5|\n",
      "|  20110043|   B04|       2003259|          Alarms|01/11/2002|01/10/2002|               Other|01/11/2002 06:01:...|1400 Block of SUT...|  SF|  94109|      B04|         03|3223|               3|       3|            3|  false|         null|        1|         CHIEF|                         2|                     4|                 2|    Western Addition|(37.7872890372638...|020110043-B04|3.4833333|\n",
      "|  20110072|   T08|       2003279|  Structure Fire|01/11/2002|01/11/2002|               Other|01/11/2002 08:03:...|  BEALE ST/FOLSOM ST|  SF|  94105|      B03|         35|2122|               3|       3|            3|  false|         null|        1|         TRUCK|                         2|                     3|                 6|Financial Distric...|(37.7886866619654...|020110072-T08|     1.75|\n",
      "|  20110125|   E33|       2003301|          Alarms|01/11/2002|01/11/2002|               Other|01/11/2002 09:46:...|0 Block of FARALL...|  SF|  94112|      B09|         33|8324|               3|       3|            3|  false|         null|        1|        ENGINE|                         2|                     9|                11|Oceanview/Merced/...|(37.7140353531157...|020110125-E33|2.7166667|\n",
      "|  20110130|   E36|       2003304|          Alarms|01/11/2002|01/11/2002|               Other|01/11/2002 09:58:...|600 Block of POLK ST|  SF|  94102|      B02|         03|3114|               3|       3|            3|  false|         null|        1|        ENGINE|                         1|                     2|                 6|          Tenderloin|(37.7826266328595...|020110130-E36|1.7833333|\n",
      "|  20110197|   E05|       2003343|Medical Incident|01/11/2002|01/11/2002|               Other|01/11/2002 12:06:...|1500 Block of WEB...|  SF|  94115|      B04|         05|3513|               3|       3|            3|  false|         null|        1|        ENGINE|                         1|                     4|                 5|           Japantown|(37.784958590666,...|020110197-E05|1.5166667|\n",
      "|  20110215|   E06|       2003348|Medical Incident|01/11/2002|01/11/2002|               Other|01/11/2002 01:08:...|DIAMOND ST/MARKET ST|  SF|  94114|      B05|         06|5415|               3|       3|            3|  false|         null|        1|        ENGINE|                         1|                     5|                 8| Castro/Upper Market|(37.7618954753708...|020110215-E06|2.7666667|\n",
      "|  20110274|   M07|       2003381|Medical Incident|01/11/2002|01/11/2002|               Other|01/11/2002 03:31:...|2700 Block of MIS...|  SF|  94110|      B06|         11|5525|               1|       1|            2|   true|         null|        1|         MEDIC|                         1|                     6|                 9|             Mission|(37.7530339738059...|020110274-M07|2.1833334|\n",
      "|  20110275|   T15|       2003382|  Structure Fire|01/11/2002|01/11/2002|               Other|01/11/2002 02:59:...|BRUNSWICK ST/GUTT...|  SF|  94112|      B09|         43|6218|               3|       3|            3|  false|         null|        1|         TRUCK|                         1|                     9|                11|           Excelsior|(37.7105545807996...|020110275-T15|      2.5|\n",
      "|  20110304|   E03|       2003399|Medical Incident|01/11/2002|01/11/2002|               Other|01/11/2002 04:22:...|1000 Block of SUT...|  SF|  94109|      B04|         03|1557|               3|       3|            3|  false|         null|        1|        ENGINE|                         1|                     4|                 3|            Nob Hill|(37.7881263034393...|020110304-E03|2.4166667|\n",
      "|  20110308|   E14|       2003403|Medical Incident|01/11/2002|01/11/2002|               Other|01/11/2002 04:18:...|100 Block of 21ST...|  SF|  94121|      B07|         14|7173|               3|       3|            3|  false|         null|        1|        ENGINE|                         1|                     7|                 1|      Outer Richmond|(37.7850084431077...|020110308-E14|     4.95|\n",
      "|  20110313|   B10|       2003408|  Structure Fire|01/11/2002|01/11/2002|               Other|01/11/2002 04:09:...|700 Block of CAPP ST|  SF|  94110|      B06|         07|5472|               3|       3|            3|  false|         null|        1|         CHIEF|                         6|                     6|                 9|             Mission|(37.7547064357942...|020110313-B10|1.4166666|\n",
      "|  20110313|    D3|       2003408|  Structure Fire|01/11/2002|01/11/2002|               Other|01/11/2002 04:09:...|700 Block of CAPP ST|  SF|  94110|      B06|         07|5472|               3|       3|            3|  false|         null|        1|         CHIEF|                         4|                     6|                 9|             Mission|(37.7547064357942...| 020110313-D3|2.5333333|\n",
      "|  20110313|   E32|       2003408|  Structure Fire|01/11/2002|01/11/2002|               Other|01/11/2002 04:09:...|700 Block of CAPP ST|  SF|  94110|      B06|         07|5472|               3|       3|            3|   true|         null|        1|        ENGINE|                         8|                     6|                 9|             Mission|(37.7547064357942...|020110313-E32|1.8833333|\n",
      "|  20110315|   RC2|       2003409|Medical Incident|01/11/2002|01/11/2002|               Other|01/11/2002 04:34:...|200 Block of LAGU...|  SF|  94116|      B08|         20|8635|               3|       3|            3|   true|         null|        1|RESCUE CAPTAIN|                         2|                     8|                 7|  West of Twin Peaks|(37.7501117393668...|020110315-RC2|     5.35|\n",
      "|  20110330|   E14|       2003417|Medical Incident|01/11/2002|01/11/2002|               Other|01/11/2002 04:51:...|BALBOA ST/PARK PR...|  SF|  94118|      B07|         31|7145|               3|       3|            3|  false|         null|        1|        ENGINE|                         1|                     7|                 1|      Inner Richmond|(37.7768682293368...|020110330-E14|      2.0|\n",
      "|  20110330|   M12|       2003417|Medical Incident|01/11/2002|01/11/2002|               Other|01/11/2002 04:51:...|BALBOA ST/PARK PR...|  SF|  94118|      B07|         31|7145|               3|       3|            3|   true|         null|        1|         MEDIC|                         2|                     7|                 1|      Inner Richmond|(37.7768682293368...|020110330-M12|1.8166667|\n",
      "+----------+------+--------------+----------------+----------+----------+--------------------+--------------------+--------------------+----+-------+---------+-----------+----+----------------+--------+-------------+-------+-------------+---------+--------------+--------------------------+----------------------+------------------+--------------------+--------------------+-------------+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[36mpath\u001b[39m: \u001b[32mString\u001b[39m = \u001b[32m\"data/sf-fire-calls.csv\"\u001b[39m\r\n",
       "\u001b[36mfireDF\u001b[39m: \u001b[32mDataFrame\u001b[39m = [CallNumber: string, UnitID: string ... 26 more fields]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val path = \"data/sf-fire-calls.csv\"\n",
    "val fireDF = spark\n",
    "        .read\n",
    "        .option(\"header\",\"true\")\n",
    "        .csv(path)\n",
    "\n",
    "println(\"le nombre de ligne est \" +fireDF.count())\n",
    "println(\"le nombre de colonne est \" +fireDF.columns.length)\n",
    "fireDF.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q4. Mettez en cache les donnees chargees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/04/25 19:04:30 WARN CacheManager: Asked to cache already cached data.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[36mres15_0\u001b[39m: \u001b[32mDataFrame\u001b[39m = [CallNumber: string, UnitID: string ... 26 more fields]\r\n",
       "\u001b[36mres15_1\u001b[39m: \u001b[32mDataFrame\u001b[39m = [CallNumber: string, UnitID: string ... 26 more fields]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fireDF.cache\n",
    "fireDF.persist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.sql.functions._\u001b[39m"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.functions._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On utilise la mise en cache quand on effectue plusieurs actions sur le même DataFrame. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q5. Supprimez tous les appels de type `Medical Incident`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hint: appliquez la methode `.filter()` a la colonne `CallType` avec l'operateur `=!=`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------+--------------+--------------+----------+----------+--------------------+--------------------+--------------------+----+-------+---------+-----------+----+----------------+--------+-------------+-------+-------------+---------+--------+--------------------------+----------------------+------------------+--------------------+--------------------+-------------+---------+\n",
      "|CallNumber|UnitID|IncidentNumber|      CallType|  CallDate| WatchDate|CallFinalDisposition|       AvailableDtTm|             Address|City|Zipcode|Battalion|StationArea| Box|OriginalPriority|Priority|FinalPriority|ALSUnit|CallTypeGroup|NumAlarms|UnitType|UnitSequenceInCallDispatch|FirePreventionDistrict|SupervisorDistrict|        Neighborhood|            Location|        RowID|    Delay|\n",
      "+----------+------+--------------+--------------+----------+----------+--------------------+--------------------+--------------------+----+-------+---------+-----------+----+----------------+--------+-------------+-------+-------------+---------+--------+--------------------------+----------------------+------------------+--------------------+--------------------+-------------+---------+\n",
      "|  20110016|   T13|       2003235|Structure Fire|01/11/2002|01/10/2002|               Other|01/11/2002 01:51:...|2000 Block of CAL...|  SF|  94109|      B04|         38|3362|               3|       3|            3|  false|         null|        1|   TRUCK|                         2|                     4|                 5|     Pacific Heights|(37.7895840679362...|020110016-T13|     2.95|\n",
      "|  20110032|   E11|       2003250|  Vehicle Fire|01/11/2002|01/10/2002|               Other|01/11/2002 04:16:...|APPLETON AV/MISSI...|  SF|  94110|      B06|         32|5626|               3|       3|            3|  false|         null|        1|  ENGINE|                         1|                     6|                 9|      Bernal Heights|(37.7388432849018...|020110032-E11|      1.5|\n",
      "|  20110043|   B04|       2003259|        Alarms|01/11/2002|01/10/2002|               Other|01/11/2002 06:01:...|1400 Block of SUT...|  SF|  94109|      B04|         03|3223|               3|       3|            3|  false|         null|        1|   CHIEF|                         2|                     4|                 2|    Western Addition|(37.7872890372638...|020110043-B04|3.4833333|\n",
      "|  20110072|   T08|       2003279|Structure Fire|01/11/2002|01/11/2002|               Other|01/11/2002 08:03:...|  BEALE ST/FOLSOM ST|  SF|  94105|      B03|         35|2122|               3|       3|            3|  false|         null|        1|   TRUCK|                         2|                     3|                 6|Financial Distric...|(37.7886866619654...|020110072-T08|     1.75|\n",
      "|  20110125|   E33|       2003301|        Alarms|01/11/2002|01/11/2002|               Other|01/11/2002 09:46:...|0 Block of FARALL...|  SF|  94112|      B09|         33|8324|               3|       3|            3|  false|         null|        1|  ENGINE|                         2|                     9|                11|Oceanview/Merced/...|(37.7140353531157...|020110125-E33|2.7166667|\n",
      "|  20110130|   E36|       2003304|        Alarms|01/11/2002|01/11/2002|               Other|01/11/2002 09:58:...|600 Block of POLK ST|  SF|  94102|      B02|         03|3114|               3|       3|            3|  false|         null|        1|  ENGINE|                         1|                     2|                 6|          Tenderloin|(37.7826266328595...|020110130-E36|1.7833333|\n",
      "|  20110275|   T15|       2003382|Structure Fire|01/11/2002|01/11/2002|               Other|01/11/2002 02:59:...|BRUNSWICK ST/GUTT...|  SF|  94112|      B09|         43|6218|               3|       3|            3|  false|         null|        1|   TRUCK|                         1|                     9|                11|           Excelsior|(37.7105545807996...|020110275-T15|      2.5|\n",
      "|  20110313|   B10|       2003408|Structure Fire|01/11/2002|01/11/2002|               Other|01/11/2002 04:09:...|700 Block of CAPP ST|  SF|  94110|      B06|         07|5472|               3|       3|            3|  false|         null|        1|   CHIEF|                         6|                     6|                 9|             Mission|(37.7547064357942...|020110313-B10|1.4166666|\n",
      "|  20110313|    D3|       2003408|Structure Fire|01/11/2002|01/11/2002|               Other|01/11/2002 04:09:...|700 Block of CAPP ST|  SF|  94110|      B06|         07|5472|               3|       3|            3|  false|         null|        1|   CHIEF|                         4|                     6|                 9|             Mission|(37.7547064357942...| 020110313-D3|2.5333333|\n",
      "|  20110313|   E32|       2003408|Structure Fire|01/11/2002|01/11/2002|               Other|01/11/2002 04:09:...|700 Block of CAPP ST|  SF|  94110|      B06|         07|5472|               3|       3|            3|   true|         null|        1|  ENGINE|                         8|                     6|                 9|             Mission|(37.7547064357942...|020110313-E32|1.8833333|\n",
      "+----------+------+--------------+--------------+----------+----------+--------------------+--------------------+--------------------+----+-------+---------+-----------+----+----------------+--------+-------------+-------+-------------+---------+--------+--------------------------+----------------------+------------------+--------------------+--------------------+-------------+---------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[36mdf_without_mi\u001b[39m: \u001b[32mDataset\u001b[39m[\u001b[32mRow\u001b[39m] = [CallNumber: string, UnitID: string ... 26 more fields]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val df_without_mi =fireDF.filter(col(\"CallType\")=!= \"Medical Incident\")\n",
    "df_without_mi.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q6. Combien de types d'appels distincts ont été passés ?**  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------+\n",
      "|Nombre de Type d'appel|\n",
      "+----------------------+\n",
      "|                    30|\n",
      "+----------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "fireDF.agg(countDistinct(\"CallType\") as \"Nombre de Type d'appel\").show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q7. Quels types d'appels  ont été passés au service d'incendie?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------------------------+\n",
      "|CallType                                    |\n",
      "+--------------------------------------------+\n",
      "|Elevator / Escalator Rescue                 |\n",
      "|Marine Fire                                 |\n",
      "|Aircraft Emergency                          |\n",
      "|Confined Space / Structure Collapse         |\n",
      "|Administrative                              |\n",
      "|Alarms                                      |\n",
      "|Odor (Strange / Unknown)                    |\n",
      "|Citizen Assist / Service Call               |\n",
      "|HazMat                                      |\n",
      "|Watercraft in Distress                      |\n",
      "|Explosion                                   |\n",
      "|Oil Spill                                   |\n",
      "|Vehicle Fire                                |\n",
      "|Suspicious Package                          |\n",
      "|Extrication / Entrapped (Machinery, Vehicle)|\n",
      "|Other                                       |\n",
      "|Outside Fire                                |\n",
      "|Traffic Collision                           |\n",
      "|Assist Police                               |\n",
      "|Gas Leak (Natural and LP Gases)             |\n",
      "|Water Rescue                                |\n",
      "|Electrical Hazard                           |\n",
      "|High Angle Rescue                           |\n",
      "|Structure Fire                              |\n",
      "|Industrial Accidents                        |\n",
      "|Medical Incident                            |\n",
      "|Mutual Aid / Assist Outside Agency          |\n",
      "|Fuel Spill                                  |\n",
      "|Smoke Investigation (Outside)               |\n",
      "|Train / Rail Incident                       |\n",
      "+--------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "fireDF.select(\"CallType\").distinct.show(30,truncate =false)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q8. Trouvez toutes les réponses ou les délais sont supérieurs à 5 minutes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hint:\n",
    "1. Renommez la colonne `Delay` -> `ReponseDelayedinMins`\n",
    "2. Retournez un nouveau DataFrame\n",
    "3. Affichez tous les appels où le temps de réponse au site d'incendie a eu un retard de plus de 5 minutes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------+--------------+-----------------------------+----------+----------+--------------------+----------------------+---------------------------+----+-------+---------+-----------+----+----------------+--------+-------------+-------+-------------+---------+------------+--------------------------+----------------------+------------------+------------------------------+-------------------------------------+-------------+---------------------+\n",
      "|CallNumber|UnitID|IncidentNumber|CallType                     |CallDate  |WatchDate |CallFinalDisposition|AvailableDtTm         |Address                    |City|Zipcode|Battalion|StationArea|Box |OriginalPriority|Priority|FinalPriority|ALSUnit|CallTypeGroup|NumAlarms|UnitType    |UnitSequenceInCallDispatch|FirePreventionDistrict|SupervisorDistrict|Neighborhood                  |Location                             |RowID        |ResponseDelayedinMins|\n",
      "+----------+------+--------------+-----------------------------+----------+----------+--------------------+----------------------+---------------------------+----+-------+---------+-----------+----+----------------+--------+-------------+-------+-------------+---------+------------+--------------------------+----------------------+------------------+------------------------------+-------------------------------------+-------------+---------------------+\n",
      "|20120147  |M38   |2003642       |Medical Incident             |01/12/2002|01/12/2002|Other               |01/12/2002 01:23:04 PM|1300 Block of HYDE ST      |SF  |94109  |B01      |41         |1564|1               |1       |2            |true   |null         |1        |MEDIC       |1                         |1                     |3                 |Russian Hill                  |(37.793235074749, -122.417915793747) |020120147-M38|6.25                 |\n",
      "|20140177  |RS1   |2004216       |Medical Incident             |01/14/2002|01/14/2002|Other               |01/14/2002 12:27:03 PM|MONTGOMERY ST/SUTTER ST    |SF  |94104  |B01      |13         |1164|3               |3       |3            |false  |null         |1        |RESCUE SQUAD|2                         |1                     |3                 |Financial District/South Beach|(37.7900390948551, -122.40218964437) |020140177-RS1|7.25                 |\n",
      "|20150056  |E35   |2004408       |Medical Incident             |01/15/2002|01/14/2002|Other               |01/15/2002 06:53:38 AM|100 Block of 7TH ST        |SF  |94103  |B03      |08         |2312|3               |3       |3            |false  |null         |1        |ENGINE      |2                         |3                     |6                 |South of Market               |(37.7790372933741, -122.41062000696) |020150056-E35|11.916667            |\n",
      "|20150265  |T16   |2004528       |Medical Incident             |01/15/2002|01/15/2002|Other               |01/15/2002 03:55:41 PM|FRANCISCO ST/MASON ST      |SF  |94133  |B01      |28         |1434|3               |3       |3            |false  |null         |1        |TRUCK       |4                         |1                     |3                 |North Beach                   |(37.8046841615796, -122.413428555065)|020150265-T16|8.633333             |\n",
      "|20150265  |UU1   |2004528       |Medical Incident             |01/15/2002|01/15/2002|Other               |01/15/2002 07:49:05 PM|FRANCISCO ST/MASON ST      |SF  |94133  |B01      |28         |1434|3               |3       |3            |false  |null         |1        |SUPPORT     |9                         |1                     |3                 |North Beach                   |(37.8046841615796, -122.413428555065)|020150265-UU1|95.28333             |\n",
      "|20150414  |M12   |2004641       |Medical Incident             |01/15/2002|01/15/2002|Other               |01/15/2002 09:46:50 PM|800 Block of CLAYTON ST    |SF  |94117  |B05      |12         |5151|2               |2       |2            |true   |null         |1        |MEDIC       |1                         |5                     |5                 |Haight Ashbury                |(37.7665307249018, -122.447986787891)|020150414-M12|7.6                  |\n",
      "|20160059  |M15   |2004721       |Medical Incident             |01/16/2002|01/15/2002|Other               |01/16/2002 08:43:45 AM|400 Block of PALMETTO AVE  |SF  |94132  |B09      |33         |8412|3               |3       |3            |false  |null         |1        |MEDIC       |3                         |9                     |7                 |Oceanview/Merced/Ingleside    |(37.7107387933676, -122.46569167278) |020160059-M15|6.133333             |\n",
      "|20170118  |E26   |2005038       |Medical Incident             |01/17/2002|01/17/2002|Other               |01/17/2002 10:32:16 AM|2900 Block of DIAMOND ST   |SF  |94131  |B09      |26         |8261|3               |3       |3            |false  |null         |1        |ENGINE      |2                         |9                     |8                 |West of Twin Peaks            |(37.7330546406059, -122.434361048267)|020170118-E26|6.9166665            |\n",
      "|20180129  |T06   |2005321       |Alarms                       |01/18/2002|01/18/2002|Other               |01/18/2002 09:40:38 AM|500 Block of VALENCIA ST   |SF  |94103  |B02      |07         |5247|3               |3       |3            |false  |null         |1        |TRUCK       |3                         |2                     |9                 |Mission                       |(37.7641127383912, -122.421739995141)|020180129-T06|6.35                 |\n",
      "|20180191  |91    |2005353       |Other                        |01/18/2002|01/18/2002|Other               |01/18/2002 12:51:51 PM|0 Block of LECH WALESA ST  |SF  |94102  |B02      |36         |0381|3               |3       |3            |false  |null         |1        |MEDIC       |1                         |2                     |6                 |Tenderloin                    |(37.7779246419176, -122.418936072128)|020180191-91 |7.983333             |\n",
      "|20180382  |M36   |2005480       |Medical Incident             |01/18/2002|01/18/2002|Other               |01/18/2002 07:33:58 PM|2700 Block of 16TH ST      |SF  |94110  |B02      |07         |5237|2               |2       |2            |true   |null         |1        |MEDIC       |1                         |2                     |9                 |Mission                       |(37.7653263612606, -122.414201573327)|020180382-M36|13.55                |\n",
      "|20190097  |RS1   |2005623       |Structure Fire               |01/19/2002|01/19/2002|Other               |01/19/2002 10:50:41 AM|100 Block of 2ND ST        |SF  |94105  |B03      |01         |2146|3               |3       |3            |true   |null         |1        |RESCUE SQUAD|2                         |3                     |6                 |Financial District/South Beach|(37.787352867216, -122.399071684966) |020190097-RS1|13.583333            |\n",
      "|20190256  |M10   |2005737       |Medical Incident             |01/19/2002|01/19/2002|Other               |01/19/2002 04:39:13 PM|3300 Block of CALIFORNIA ST|SF  |94118  |B04      |10         |4421|1               |1       |2            |false  |null         |1        |MEDIC       |1                         |4                     |2                 |Presidio Heights              |(37.7869977461494, -122.449304455831)|020190256-M10|6.5333333            |\n",
      "|20210014  |M29   |2006124       |Medical Incident             |01/21/2002|01/20/2002|Other               |01/21/2002 02:00:52 AM|300 Block of BALTIMORE WAY |SF  |94112  |B09      |43         |6228|2               |2       |2            |true   |null         |1        |MEDIC       |1                         |9                     |11                |Excelsior                     |(37.7087981055096, -122.437985362628)|020210014-M29|8.15                 |\n",
      "|20220088  |E36   |2006426       |Citizen Assist / Service Call|01/22/2002|01/22/2002|Other               |01/22/2002 09:39:52 AM|700 Block of EDDY ST       |SF  |94102  |B02      |03         |3163|3               |3       |3            |false  |null         |1        |ENGINE      |1                         |2                     |6                 |Tenderloin                    |(37.782943523582, -122.419988441304) |020220088-E36|6.6                  |\n",
      "|20220147  |E28   |2006461       |Medical Incident             |01/22/2002|01/22/2002|Other               |01/22/2002 10:37:00 AM|900 Block of GREEN ST      |SF  |94133  |B01      |02         |1441|3               |3       |3            |false  |null         |1        |ENGINE      |5                         |1                     |3                 |Nob Hill                      |(37.798689014807, -122.415061918203) |020220147-E28|7.0666666            |\n",
      "|20220217  |M14   |2006513       |Medical Incident             |01/22/2002|01/22/2002|Other               |null                  |2300 Block of 24TH AVE     |SF  |94116  |B08      |40         |7461|1               |1       |2            |true   |null         |1        |MEDIC       |2                         |8                     |4                 |Sunset/Parkside               |(37.7437447222589, -122.481184434413)|020220217-M14|13.4                 |\n",
      "|20220356  |E11   |2006604       |Medical Incident             |01/22/2002|01/22/2002|Other               |01/22/2002 04:24:15 PM|2800 Block of MISSION ST   |SF  |94110  |B06      |11         |5525|2               |2       |2            |false  |null         |1        |ENGINE      |1                         |6                     |9                 |Mission                       |(37.7514438314246, -122.41829760412) |020220356-E11|8.716666             |\n",
      "|20230040  |M03   |2006746       |Medical Incident             |01/23/2002|01/22/2002|Other               |01/23/2002 06:42:45 AM|700 Block of LARKIN ST     |SF  |94109  |B04      |03         |1642|2               |2       |2            |true   |null         |1        |MEDIC       |1                         |2                     |6                 |Tenderloin                    |(37.7849474956973, -122.417785340586)|020230040-M03|7.9333334            |\n",
      "|20230144  |T06   |2006795       |Medical Incident             |01/23/2002|01/23/2002|Other               |01/23/2002 09:56:44 AM|CHURCH ST/DUBOCE AV        |SF  |94114  |B02      |06         |3525|3               |3       |3            |false  |null         |1        |TRUCK       |1                         |2                     |8                 |Hayes Valley                  |(37.7694561276975, -122.429127986942)|020230144-T06|6.4666667            |\n",
      "+----------+------+--------------+-----------------------------+----------+----------+--------------------+----------------------+---------------------------+----+-------+---------+-----------+----+----------------+--------+-------------+-------+-------------+---------+------------+--------------------------+----------------------+------------------+------------------------------+-------------------------------------+-------------+---------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[36mfireWithDelayRename\u001b[39m: \u001b[32mDataFrame\u001b[39m = [CallNumber: string, UnitID: string ... 26 more fields]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val fireWithDelayRename = fireDF.withColumnRenamed(\"Delay\", \"ResponseDelayedinMins\")\n",
    "\n",
    "\n",
    "fireWithDelayRename.filter(col(\"ResponseDelayedinMins\") > 5).show(false)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q9. Convertissez les colonnes dates en timestamp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hint:\n",
    "* `CallDate` -> `IncidentDate`\n",
    "* `WatchDate` -> `OnWatchDate`\n",
    "* `AvailableDtTm` -> `AvailableDtTS`\n",
    "exemple code pour le cas de `CallDate`:\n",
    "`dataframe.withColumn(\"IncidentDate\", to_timestamp(col(\"CallDate\"), \"MM/dd/yyyy\")).drop(\"CallDate\")`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mdfWithEncondedDate\u001b[39m: \u001b[32mDataFrame\u001b[39m = [CallNumber: string, UnitID: string ... 26 more fields]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val dfWithEncondedDate =(fireDF.withColumn(\"IncidentDate\",to_timestamp(col(\"CallDate\"), \"MM/dd/yyyy\")).drop(\"CallDate\")\n",
    "                 .withColumn(\"OnWatchDate\",to_timestamp(col(\"WatchDate\"), \"MM/dd/yyyy\")).drop(\"WatchDate\")\n",
    "                 .withColumn(\"AvailableDtTS\",to_timestamp(col(\"AvailableDtTm\"), \"MM/dd/yyyy\")).drop(\"AvailableDtTm\")\n",
    "                 .withColumnRenamed(\"Delay\", \"ResponseDelayedinMins\")\n",
    ")\n",
    "\n",
    "                 \n",
    "               "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q10. Quels sont les types d'appels les plus courants?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------------+------+\n",
      "|CallType                     |count |\n",
      "+-----------------------------+------+\n",
      "|Medical Incident             |113794|\n",
      "|Structure Fire               |23319 |\n",
      "|Alarms                       |19406 |\n",
      "|Traffic Collision            |7013  |\n",
      "|Citizen Assist / Service Call|2524  |\n",
      "+-----------------------------+------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfWithEncondedDate.groupBy(\"CallType\")\n",
    ".count()\n",
    ".orderBy(col(\"count\").desc)\n",
    ".show(5, truncate =false)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q11. Quels sont les boites postales rencontrées dans les appels les plus courants?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+-------+-----+\n",
      "|CallType        |Zipcode|count|\n",
      "+----------------+-------+-----+\n",
      "|Medical Incident|94102  |16130|\n",
      "|Medical Incident|94103  |14775|\n",
      "|Medical Incident|94110  |9995 |\n",
      "|Medical Incident|94109  |9479 |\n",
      "|Medical Incident|94124  |5885 |\n",
      "|Medical Incident|94112  |5630 |\n",
      "|Medical Incident|94115  |4785 |\n",
      "|Medical Incident|94122  |4323 |\n",
      "|Medical Incident|94107  |4284 |\n",
      "|Medical Incident|94133  |3977 |\n",
      "+----------------+-------+-----+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[36mtopZipcodes\u001b[39m: \u001b[32mDataset\u001b[39m[\u001b[32mRow\u001b[39m] = [CallType: string, Zipcode: string ... 1 more field]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val topZipcodes = fireDF.groupBy(\"CallType\", \"Zipcode\")\n",
    "  .count()\n",
    "  .orderBy(desc(\"count\"))\n",
    "  .limit(10)\n",
    "\n",
    "  topZipcodes.show(false)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q12. Quels sont les quartiers de San Francisco dont les codes postaux sont `94102` et `94103`?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------+-------+\n",
      "|Neighborhood                  |Zipcode|\n",
      "+------------------------------+-------+\n",
      "|Mission Bay                   |94103  |\n",
      "|Financial District/South Beach|94103  |\n",
      "|Castro/Upper Market           |94103  |\n",
      "|Western Addition              |94102  |\n",
      "|Nob Hill                      |94102  |\n",
      "|South of Market               |94103  |\n",
      "|Potrero Hill                  |94103  |\n",
      "|Hayes Valley                  |94103  |\n",
      "|South of Market               |94102  |\n",
      "|Tenderloin                    |94102  |\n",
      "|Tenderloin                    |94103  |\n",
      "|Mission                       |94103  |\n",
      "|Financial District/South Beach|94102  |\n",
      "|Hayes Valley                  |94102  |\n",
      "+------------------------------+-------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[36mfilteredDF\u001b[39m: \u001b[32mDataset\u001b[39m[\u001b[32mRow\u001b[39m] = [CallNumber: string, UnitID: string ... 26 more fields]\r\n",
       "\u001b[36mneighborhoods\u001b[39m: \u001b[32mDataset\u001b[39m[\u001b[32mRow\u001b[39m] = [Neighborhood: string, Zipcode: string]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val filteredDF = fireDF.filter(col(\"Zipcode\").isin(\"94102\", \"94103\"))\n",
    "\n",
    "val neighborhoods = filteredDF.select(\"Neighborhood\",\"Zipcode\").distinct()\n",
    "\n",
    "neighborhoods.show(false)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q13. Determinez le nombre total d'appels, ainsi que la moyenne, le minimum et le maximum du temps de réponse des appels?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---------------------+\n",
      "|summary|ResponseDelayedinMins|\n",
      "+-------+---------------------+\n",
      "|  count|               175296|\n",
      "|   mean|   3.8923641541750413|\n",
      "| stddev|    9.378286170882717|\n",
      "|    min|          0.016666668|\n",
      "|    max|                 99.9|\n",
      "+-------+---------------------+\n",
      "\n",
      "root\n",
      " |-- CallNumber: string (nullable = true)\n",
      " |-- UnitID: string (nullable = true)\n",
      " |-- IncidentNumber: string (nullable = true)\n",
      " |-- CallType: string (nullable = true)\n",
      " |-- CallDate: string (nullable = true)\n",
      " |-- WatchDate: string (nullable = true)\n",
      " |-- CallFinalDisposition: string (nullable = true)\n",
      " |-- AvailableDtTm: string (nullable = true)\n",
      " |-- Address: string (nullable = true)\n",
      " |-- City: string (nullable = true)\n",
      " |-- Zipcode: string (nullable = true)\n",
      " |-- Battalion: string (nullable = true)\n",
      " |-- StationArea: string (nullable = true)\n",
      " |-- Box: string (nullable = true)\n",
      " |-- OriginalPriority: string (nullable = true)\n",
      " |-- Priority: string (nullable = true)\n",
      " |-- FinalPriority: string (nullable = true)\n",
      " |-- ALSUnit: string (nullable = true)\n",
      " |-- CallTypeGroup: string (nullable = true)\n",
      " |-- NumAlarms: string (nullable = true)\n",
      " |-- UnitType: string (nullable = true)\n",
      " |-- UnitSequenceInCallDispatch: string (nullable = true)\n",
      " |-- FirePreventionDistrict: string (nullable = true)\n",
      " |-- SupervisorDistrict: string (nullable = true)\n",
      " |-- Neighborhood: string (nullable = true)\n",
      " |-- Location: string (nullable = true)\n",
      " |-- RowID: string (nullable = true)\n",
      " |-- ResponseDelayedinMins: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "fireWithDelayRename.describe(\"ResponseDelayedinMins\").show()\n",
    "fireWithDelayRename.printSchema()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q14. Combien d'années distinctes trouve t-on dans ce Dataset? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hint: Appliquer la fonction `year()` a la colonne `IncidentDate`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nombre d'années distinctes dans le Dataset : 19\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.sql.functions.year\n",
       "\n",
       "\u001b[39m\r\n",
       "\u001b[36mnumYears\u001b[39m: \u001b[32mLong\u001b[39m = \u001b[32m19L\u001b[39m"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.functions.year\n",
    "\n",
    "val numYears = dfWithEncondedDate.select(year(col(\"IncidentDate\")).alias(\"year\"))\n",
    "                    .distinct()\n",
    "                    .count()\n",
    "println(s\"Nombre d'années distinctes dans le Dataset : $numYears\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q15. Quelle semaine de l'année 2018 a eu le plus d'appels d'incendie?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+\n",
      "|WeekOfYear|TotalCalls|\n",
      "+----------+----------+\n",
      "|        22|       259|\n",
      "+----------+----------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.sql.functions.weekofyear\n",
       "\n",
       "\u001b[39m\r\n",
       "\u001b[36mfireDFwithWeekOfYear\u001b[39m: \u001b[32mDataFrame\u001b[39m = [CallNumber: string, UnitID: string ... 27 more fields]\r\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.sql.functions.max\n",
       "\n",
       "\u001b[39m\r\n",
       "\u001b[36mcallsByWeek\u001b[39m: \u001b[32mDataset\u001b[39m[\u001b[32mRow\u001b[39m] = [WeekOfYear: int, TotalCalls: bigint]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.functions.weekofyear\n",
    "\n",
    "val fireDFwithWeekOfYear = dfWithEncondedDate.withColumn(\"IncidentDate\", to_timestamp(col(\"IncidentDate\"), \"MM/dd/yyyy\"))\n",
    "                                  .withColumn(\"WeekOfYear\", weekofyear(col(\"IncidentDate\")))\n",
    "import org.apache.spark.sql.functions.max\n",
    "\n",
    "val callsByWeek = fireDFwithWeekOfYear.filter(year(col(\"IncidentDate\")) === 2018)\n",
    "                                      .groupBy(\"WeekOfYear\")\n",
    "                                      .agg(count(\"*\").alias(\"TotalCalls\"))\n",
    "                                      .sort(desc(\"TotalCalls\"))\n",
    "\n",
    "callsByWeek.show(1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q16. Quels sont les quartiers de San Francisco qui ont connu le pire temps de réponse en 2018?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---------------------+\n",
      "|        Neighborhood|ResponseDelayedinMins|\n",
      "+--------------------+---------------------+\n",
      "|     South of Market|             94.71667|\n",
      "|Bayview Hunters P...|            92.816666|\n",
      "|     South of Market|            91.666664|\n",
      "|      Inner Richmond|            90.433334|\n",
      "|        Russian Hill|             9.983334|\n",
      "|             Mission|                 9.95|\n",
      "|     South of Market|             9.933333|\n",
      "|    Golden Gate Park|             9.933333|\n",
      "|        Potrero Hill|             9.916667|\n",
      "|           Chinatown|                  9.9|\n",
      "+--------------------+---------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.sql.functions._\n",
       "\n",
       " \u001b[39m"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.functions._\n",
    "\n",
    " dfWithEncondedDate.filter(year(col(\"IncidentDate\")) === 2018)\n",
    "  .select(\"Neighborhood\",\"ResponseDelayedinMins\")\n",
    "  .orderBy(desc(\"ResponseDelayedinMins\"))\n",
    "  .show(10)\n",
    " \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q17. Stocker les données sous format de fichiers Parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/04/25 19:08:51 ERROR Executor: Exception in task 0.0 in stage 48.0 (TID 1423)\n",
      "java.io.IOException: (null) entry in command string: null chmod 0644 C:\\Users\\KH\\Desktop\\Master 1\\BigData\\bigdata\\bigdata\\data\\parquet_data_1\\_temporary\\0\\_temporary\\attempt_20230425190851_0048_m_000000_1423\\part-00000-4560d31a-9d8e-40ee-8898-bf5b767531ad-c000.snappy.parquet\n",
      "\tat org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:762)\n",
      "\tat org.apache.hadoop.util.Shell.execCommand(Shell.java:859)\n",
      "\tat org.apache.hadoop.util.Shell.execCommand(Shell.java:842)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.setPermission(RawLocalFileSystem.java:661)\n",
      "\tat org.apache.hadoop.fs.ChecksumFileSystem$1.apply(ChecksumFileSystem.java:501)\n",
      "\tat org.apache.hadoop.fs.ChecksumFileSystem$FsOperation.run(ChecksumFileSystem.java:482)\n",
      "\tat org.apache.hadoop.fs.ChecksumFileSystem.setPermission(ChecksumFileSystem.java:498)\n",
      "\tat org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:467)\n",
      "\tat org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:433)\n",
      "\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:908)\n",
      "\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:889)\n",
      "\tat org.apache.parquet.hadoop.util.HadoopOutputFile.create(HadoopOutputFile.java:74)\n",
      "\tat org.apache.parquet.hadoop.ParquetFileWriter.<init>(ParquetFileWriter.java:248)\n",
      "\tat org.apache.parquet.hadoop.ParquetOutputFormat.getRecordWriter(ParquetOutputFormat.java:390)\n",
      "\tat org.apache.parquet.hadoop.ParquetOutputFormat.getRecordWriter(ParquetOutputFormat.java:349)\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetOutputWriter.<init>(ParquetOutputWriter.scala:37)\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$$anon$1.newInstance(ParquetFileFormat.scala:151)\n",
      "\tat org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.newOutputWriter(FileFormatDataWriter.scala:123)\n",
      "\tat org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.<init>(FileFormatDataWriter.scala:108)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:236)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$15(FileFormatWriter.scala:177)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:123)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:411)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "23/04/25 19:08:51 WARN TaskSetManager: Lost task 0.0 in stage 48.0 (TID 1423, localhost, executor driver): java.io.IOException: (null) entry in command string: null chmod 0644 C:\\Users\\KH\\Desktop\\Master 1\\BigData\\bigdata\\bigdata\\data\\parquet_data_1\\_temporary\\0\\_temporary\\attempt_20230425190851_0048_m_000000_1423\\part-00000-4560d31a-9d8e-40ee-8898-bf5b767531ad-c000.snappy.parquet\n",
      "\tat org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:762)\n",
      "\tat org.apache.hadoop.util.Shell.execCommand(Shell.java:859)\n",
      "\tat org.apache.hadoop.util.Shell.execCommand(Shell.java:842)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.setPermission(RawLocalFileSystem.java:661)\n",
      "\tat org.apache.hadoop.fs.ChecksumFileSystem$1.apply(ChecksumFileSystem.java:501)\n",
      "\tat org.apache.hadoop.fs.ChecksumFileSystem$FsOperation.run(ChecksumFileSystem.java:482)\n",
      "\tat org.apache.hadoop.fs.ChecksumFileSystem.setPermission(ChecksumFileSystem.java:498)\n",
      "\tat org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:467)\n",
      "\tat org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:433)\n",
      "\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:908)\n",
      "\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:889)\n",
      "\tat org.apache.parquet.hadoop.util.HadoopOutputFile.create(HadoopOutputFile.java:74)\n",
      "\tat org.apache.parquet.hadoop.ParquetFileWriter.<init>(ParquetFileWriter.java:248)\n",
      "\tat org.apache.parquet.hadoop.ParquetOutputFormat.getRecordWriter(ParquetOutputFormat.java:390)\n",
      "\tat org.apache.parquet.hadoop.ParquetOutputFormat.getRecordWriter(ParquetOutputFormat.java:349)\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetOutputWriter.<init>(ParquetOutputWriter.scala:37)\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$$anon$1.newInstance(ParquetFileFormat.scala:151)\n",
      "\tat org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.newOutputWriter(FileFormatDataWriter.scala:123)\n",
      "\tat org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.<init>(FileFormatDataWriter.scala:108)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:236)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$15(FileFormatWriter.scala:177)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:123)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:411)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "\n",
      "23/04/25 19:08:51 ERROR TaskSetManager: Task 0 in stage 48.0 failed 1 times; aborting job\n",
      "23/04/25 19:08:51 ERROR FileFormatWriter: Aborting job 1b839044-687e-48e1-a57a-ff269dae0b53.\n",
      "org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 48.0 failed 1 times, most recent failure: Lost task 0.0 in stage 48.0 (TID 1423, localhost, executor driver): java.io.IOException: (null) entry in command string: null chmod 0644 C:\\Users\\KH\\Desktop\\Master 1\\BigData\\bigdata\\bigdata\\data\\parquet_data_1\\_temporary\\0\\_temporary\\attempt_20230425190851_0048_m_000000_1423\\part-00000-4560d31a-9d8e-40ee-8898-bf5b767531ad-c000.snappy.parquet\n",
      "\tat org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:762)\n",
      "\tat org.apache.hadoop.util.Shell.execCommand(Shell.java:859)\n",
      "\tat org.apache.hadoop.util.Shell.execCommand(Shell.java:842)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.setPermission(RawLocalFileSystem.java:661)\n",
      "\tat org.apache.hadoop.fs.ChecksumFileSystem$1.apply(ChecksumFileSystem.java:501)\n",
      "\tat org.apache.hadoop.fs.ChecksumFileSystem$FsOperation.run(ChecksumFileSystem.java:482)\n",
      "\tat org.apache.hadoop.fs.ChecksumFileSystem.setPermission(ChecksumFileSystem.java:498)\n",
      "\tat org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:467)\n",
      "\tat org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:433)\n",
      "\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:908)\n",
      "\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:889)\n",
      "\tat org.apache.parquet.hadoop.util.HadoopOutputFile.create(HadoopOutputFile.java:74)\n",
      "\tat org.apache.parquet.hadoop.ParquetFileWriter.<init>(ParquetFileWriter.java:248)\n",
      "\tat org.apache.parquet.hadoop.ParquetOutputFormat.getRecordWriter(ParquetOutputFormat.java:390)\n",
      "\tat org.apache.parquet.hadoop.ParquetOutputFormat.getRecordWriter(ParquetOutputFormat.java:349)\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetOutputWriter.<init>(ParquetOutputWriter.scala:37)\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$$anon$1.newInstance(ParquetFileFormat.scala:151)\n",
      "\tat org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.newOutputWriter(FileFormatDataWriter.scala:123)\n",
      "\tat org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.<init>(FileFormatDataWriter.scala:108)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:236)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$15(FileFormatWriter.scala:177)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:123)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:411)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "\n",
      "Driver stacktrace:\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:1891)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:1879)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:1878)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n",
      "\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1878)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:927)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:927)\n",
      "\tat scala.Option.foreach(Option.scala:407)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:927)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2112)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2061)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2050)\n",
      "\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:738)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2061)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:167)\n",
      "\tat org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:170)\n",
      "\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:104)\n",
      "\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:102)\n",
      "\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.doExecute(commands.scala:122)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:131)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:155)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:127)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:83)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:81)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.$anonfun$runCommand$1(DataFrameWriter.scala:676)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:80)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:127)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:75)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:676)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:290)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:271)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:229)\n",
      "\tat ammonite.$sess.cmd31$Helper.<init>(cmd31.sc:1)\n",
      "\tat ammonite.$sess.cmd31$.<init>(cmd31.sc:7)\n",
      "\tat ammonite.$sess.cmd31$.<clinit>(cmd31.sc)\n",
      "\tat ammonite.$sess.cmd31.$main(cmd31.sc)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
      "\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n",
      "\tat ammonite.runtime.Evaluator$$anon$1.$anonfun$evalMain$1(Evaluator.scala:108)\n",
      "\tat ammonite.util.Util$.withContextClassloader(Util.scala:24)\n",
      "\tat ammonite.runtime.Evaluator$$anon$1.evalMain(Evaluator.scala:90)\n",
      "\tat ammonite.runtime.Evaluator$$anon$1.$anonfun$processLine$2(Evaluator.scala:127)\n",
      "\tat ammonite.util.Catching.map(Res.scala:117)\n",
      "\tat ammonite.runtime.Evaluator$$anon$1.$anonfun$processLine$1(Evaluator.scala:121)\n",
      "\tat ammonite.util.Res$Success.flatMap(Res.scala:62)\n",
      "\tat ammonite.runtime.Evaluator$$anon$1.processLine(Evaluator.scala:120)\n",
      "\tat ammonite.interp.Interpreter.$anonfun$evaluateLine$4(Interpreter.scala:295)\n",
      "\tat ammonite.util.Res$Success.flatMap(Res.scala:62)\n",
      "\tat ammonite.interp.Interpreter.$anonfun$evaluateLine$2(Interpreter.scala:281)\n",
      "\tat ammonite.util.Catching.flatMap(Res.scala:115)\n",
      "\tat ammonite.interp.Interpreter.evaluateLine(Interpreter.scala:280)\n",
      "\tat ammonite.interp.Interpreter.$anonfun$processLine$6(Interpreter.scala:268)\n",
      "\tat ammonite.util.Res$Success.flatMap(Res.scala:62)\n",
      "\tat ammonite.interp.Interpreter.$anonfun$processLine$4(Interpreter.scala:251)\n",
      "\tat ammonite.util.Res$Success.flatMap(Res.scala:62)\n",
      "\tat ammonite.interp.Interpreter.$anonfun$processLine$2(Interpreter.scala:244)\n",
      "\tat ammonite.util.Catching.flatMap(Res.scala:115)\n",
      "\tat ammonite.interp.Interpreter.processLine(Interpreter.scala:243)\n",
      "\tat almond.Execute.$anonfun$ammResult$11(Execute.scala:238)\n",
      "\tat almond.internals.CaptureImpl.$anonfun$apply$2(CaptureImpl.scala:53)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat scala.Console$.withErr(Console.scala:196)\n",
      "\tat almond.internals.CaptureImpl.$anonfun$apply$1(CaptureImpl.scala:45)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat scala.Console$.withOut(Console.scala:167)\n",
      "\tat almond.internals.CaptureImpl.apply(CaptureImpl.scala:45)\n",
      "\tat almond.Execute.capturingOutput(Execute.scala:166)\n",
      "\tat almond.Execute.$anonfun$ammResult$10(Execute.scala:225)\n",
      "\tat almond.Execute.$anonfun$withClientStdin$1(Execute.scala:146)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat scala.Console$.withIn(Console.scala:230)\n",
      "\tat almond.Execute.withClientStdin(Execute.scala:142)\n",
      "\tat almond.Execute.$anonfun$ammResult$9(Execute.scala:225)\n",
      "\tat almond.Execute.withInputManager(Execute.scala:134)\n",
      "\tat almond.Execute.$anonfun$ammResult$8(Execute.scala:224)\n",
      "\tat ammonite.repl.Signaller.apply(Signaller.scala:28)\n",
      "\tat almond.Execute.interruptible(Execute.scala:183)\n",
      "\tat almond.Execute.$anonfun$ammResult$7(Execute.scala:223)\n",
      "\tat ammonite.util.Res$Success.flatMap(Res.scala:62)\n",
      "\tat almond.Execute.$anonfun$ammResult$1(Execute.scala:214)\n",
      "\tat almond.Execute.withOutputHandler(Execute.scala:157)\n",
      "\tat almond.Execute.ammResult(Execute.scala:214)\n",
      "\tat almond.Execute.apply(Execute.scala:311)\n",
      "\tat almond.ScalaInterpreter.execute(ScalaInterpreter.scala:127)\n",
      "\tat almond.interpreter.InterpreterToIOInterpreter.$anonfun$execute$2(InterpreterToIOInterpreter.scala:69)\n",
      "\tat cats.effect.internals.IORunLoop$.cats$effect$internals$IORunLoop$$loop(IORunLoop.scala:87)\n",
      "\tat cats.effect.internals.IORunLoop$RestartCallback.signal(IORunLoop.scala:366)\n",
      "\tat cats.effect.internals.IORunLoop$RestartCallback.apply(IORunLoop.scala:387)\n",
      "\tat cats.effect.internals.IORunLoop$RestartCallback.apply(IORunLoop.scala:330)\n",
      "\tat cats.effect.internals.IOShift$Tick.run(IOShift.scala:36)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "Caused by: java.io.IOException: (null) entry in command string: null chmod 0644 C:\\Users\\KH\\Desktop\\Master 1\\BigData\\bigdata\\bigdata\\data\\parquet_data_1\\_temporary\\0\\_temporary\\attempt_20230425190851_0048_m_000000_1423\\part-00000-4560d31a-9d8e-40ee-8898-bf5b767531ad-c000.snappy.parquet\n",
      "\tat org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:762)\n",
      "\tat org.apache.hadoop.util.Shell.execCommand(Shell.java:859)\n",
      "\tat org.apache.hadoop.util.Shell.execCommand(Shell.java:842)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.setPermission(RawLocalFileSystem.java:661)\n",
      "\tat org.apache.hadoop.fs.ChecksumFileSystem$1.apply(ChecksumFileSystem.java:501)\n",
      "\tat org.apache.hadoop.fs.ChecksumFileSystem$FsOperation.run(ChecksumFileSystem.java:482)\n",
      "\tat org.apache.hadoop.fs.ChecksumFileSystem.setPermission(ChecksumFileSystem.java:498)\n",
      "\tat org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:467)\n",
      "\tat org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:433)\n",
      "\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:908)\n",
      "\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:889)\n",
      "\tat org.apache.parquet.hadoop.util.HadoopOutputFile.create(HadoopOutputFile.java:74)\n",
      "\tat org.apache.parquet.hadoop.ParquetFileWriter.<init>(ParquetFileWriter.java:248)\n",
      "\tat org.apache.parquet.hadoop.ParquetOutputFormat.getRecordWriter(ParquetOutputFormat.java:390)\n",
      "\tat org.apache.parquet.hadoop.ParquetOutputFormat.getRecordWriter(ParquetOutputFormat.java:349)\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetOutputWriter.<init>(ParquetOutputWriter.scala:37)\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$$anon$1.newInstance(ParquetFileFormat.scala:151)\n",
      "\tat org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.newOutputWriter(FileFormatDataWriter.scala:123)\n",
      "\tat org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.<init>(FileFormatDataWriter.scala:108)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:236)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$15(FileFormatWriter.scala:177)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:123)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:411)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\n",
      "\t... 3 more\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31morg.apache.spark.SparkException: Job aborted.\u001b[39m\r\n  org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(\u001b[32mFileFormatWriter.scala\u001b[39m:\u001b[32m198\u001b[39m)\r\n  org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(\u001b[32mInsertIntoHadoopFsRelationCommand.scala\u001b[39m:\u001b[32m170\u001b[39m)\r\n  org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(\u001b[32mcommands.scala\u001b[39m:\u001b[32m104\u001b[39m)\r\n  org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(\u001b[32mcommands.scala\u001b[39m:\u001b[32m102\u001b[39m)\r\n  org.apache.spark.sql.execution.command.DataWritingCommandExec.doExecute(\u001b[32mcommands.scala\u001b[39m:\u001b[32m122\u001b[39m)\r\n  org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(\u001b[32mSparkPlan.scala\u001b[39m:\u001b[32m131\u001b[39m)\r\n  org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(\u001b[32mSparkPlan.scala\u001b[39m:\u001b[32m155\u001b[39m)\r\n  org.apache.spark.rdd.RDDOperationScope$.withScope(\u001b[32mRDDOperationScope.scala\u001b[39m:\u001b[32m151\u001b[39m)\r\n  org.apache.spark.sql.execution.SparkPlan.executeQuery(\u001b[32mSparkPlan.scala\u001b[39m:\u001b[32m152\u001b[39m)\r\n  org.apache.spark.sql.execution.SparkPlan.execute(\u001b[32mSparkPlan.scala\u001b[39m:\u001b[32m127\u001b[39m)\r\n  org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(\u001b[32mQueryExecution.scala\u001b[39m:\u001b[32m83\u001b[39m)\r\n  org.apache.spark.sql.execution.QueryExecution.toRdd(\u001b[32mQueryExecution.scala\u001b[39m:\u001b[32m81\u001b[39m)\r\n  org.apache.spark.sql.DataFrameWriter.$anonfun$runCommand$1(\u001b[32mDataFrameWriter.scala\u001b[39m:\u001b[32m676\u001b[39m)\r\n  org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(\u001b[32mSQLExecution.scala\u001b[39m:\u001b[32m80\u001b[39m)\r\n  org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(\u001b[32mSQLExecution.scala\u001b[39m:\u001b[32m127\u001b[39m)\r\n  org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(\u001b[32mSQLExecution.scala\u001b[39m:\u001b[32m75\u001b[39m)\r\n  org.apache.spark.sql.DataFrameWriter.runCommand(\u001b[32mDataFrameWriter.scala\u001b[39m:\u001b[32m676\u001b[39m)\r\n  org.apache.spark.sql.DataFrameWriter.saveToV1Source(\u001b[32mDataFrameWriter.scala\u001b[39m:\u001b[32m290\u001b[39m)\r\n  org.apache.spark.sql.DataFrameWriter.save(\u001b[32mDataFrameWriter.scala\u001b[39m:\u001b[32m271\u001b[39m)\r\n  org.apache.spark.sql.DataFrameWriter.save(\u001b[32mDataFrameWriter.scala\u001b[39m:\u001b[32m229\u001b[39m)\r\n  ammonite.$sess.cmd31$Helper.<init>(\u001b[32mcmd31.sc\u001b[39m:\u001b[32m1\u001b[39m)\r\n  ammonite.$sess.cmd31$.<init>(\u001b[32mcmd31.sc\u001b[39m:\u001b[32m7\u001b[39m)\r\n  ammonite.$sess.cmd31$.<clinit>(\u001b[32mcmd31.sc\u001b[39m:\u001b[32m-1\u001b[39m)\r\n\u001b[31morg.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 48.0 failed 1 times, most recent failure: Lost task 0.0 in stage 48.0 (TID 1423, localhost, executor driver): java.io.IOException: (null) entry in command string: null chmod 0644 C:\\Users\\KH\\Desktop\\Master 1\\BigData\\bigdata\\bigdata\\data\\parquet_data_1\\_temporary\\0\\_temporary\\attempt_20230425190851_0048_m_000000_1423\\part-00000-4560d31a-9d8e-40ee-8898-bf5b767531ad-c000.snappy.parquet\r\n\tat org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:762)\r\n\tat org.apache.hadoop.util.Shell.execCommand(Shell.java:859)\r\n\tat org.apache.hadoop.util.Shell.execCommand(Shell.java:842)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.setPermission(RawLocalFileSystem.java:661)\r\n\tat org.apache.hadoop.fs.ChecksumFileSystem$1.apply(ChecksumFileSystem.java:501)\r\n\tat org.apache.hadoop.fs.ChecksumFileSystem$FsOperation.run(ChecksumFileSystem.java:482)\r\n\tat org.apache.hadoop.fs.ChecksumFileSystem.setPermission(ChecksumFileSystem.java:498)\r\n\tat org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:467)\r\n\tat org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:433)\r\n\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:908)\r\n\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:889)\r\n\tat org.apache.parquet.hadoop.util.HadoopOutputFile.create(HadoopOutputFile.java:74)\r\n\tat org.apache.parquet.hadoop.ParquetFileWriter.<init>(ParquetFileWriter.java:248)\r\n\tat org.apache.parquet.hadoop.ParquetOutputFormat.getRecordWriter(ParquetOutputFormat.java:390)\r\n\tat org.apache.parquet.hadoop.ParquetOutputFormat.getRecordWriter(ParquetOutputFormat.java:349)\r\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetOutputWriter.<init>(ParquetOutputWriter.scala:37)\r\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$$anon$1.newInstance(ParquetFileFormat.scala:151)\r\n\tat org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.newOutputWriter(FileFormatDataWriter.scala:123)\r\n\tat org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.<init>(FileFormatDataWriter.scala:108)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:236)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$15(FileFormatWriter.scala:177)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:123)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:411)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\r\n\tat java.base/java.lang.Thread.run(Thread.java:829)\r\n\nDriver stacktrace:\u001b[39m\r\n  org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(\u001b[32mDAGScheduler.scala\u001b[39m:\u001b[32m1891\u001b[39m)\r\n  org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(\u001b[32mDAGScheduler.scala\u001b[39m:\u001b[32m1879\u001b[39m)\r\n  org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(\u001b[32mDAGScheduler.scala\u001b[39m:\u001b[32m1878\u001b[39m)\r\n  scala.collection.mutable.ResizableArray.foreach(\u001b[32mResizableArray.scala\u001b[39m:\u001b[32m62\u001b[39m)\r\n  scala.collection.mutable.ResizableArray.foreach$(\u001b[32mResizableArray.scala\u001b[39m:\u001b[32m55\u001b[39m)\r\n  scala.collection.mutable.ArrayBuffer.foreach(\u001b[32mArrayBuffer.scala\u001b[39m:\u001b[32m49\u001b[39m)\r\n  org.apache.spark.scheduler.DAGScheduler.abortStage(\u001b[32mDAGScheduler.scala\u001b[39m:\u001b[32m1878\u001b[39m)\r\n  org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(\u001b[32mDAGScheduler.scala\u001b[39m:\u001b[32m927\u001b[39m)\r\n  org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(\u001b[32mDAGScheduler.scala\u001b[39m:\u001b[32m927\u001b[39m)\r\n  scala.Option.foreach(\u001b[32mOption.scala\u001b[39m:\u001b[32m407\u001b[39m)\r\n  org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(\u001b[32mDAGScheduler.scala\u001b[39m:\u001b[32m927\u001b[39m)\r\n  org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(\u001b[32mDAGScheduler.scala\u001b[39m:\u001b[32m2112\u001b[39m)\r\n  org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(\u001b[32mDAGScheduler.scala\u001b[39m:\u001b[32m2061\u001b[39m)\r\n  org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(\u001b[32mDAGScheduler.scala\u001b[39m:\u001b[32m2050\u001b[39m)\r\n  org.apache.spark.util.EventLoop$$anon$1.run(\u001b[32mEventLoop.scala\u001b[39m:\u001b[32m49\u001b[39m)\r\n  org.apache.spark.scheduler.DAGScheduler.runJob(\u001b[32mDAGScheduler.scala\u001b[39m:\u001b[32m738\u001b[39m)\r\n  org.apache.spark.SparkContext.runJob(\u001b[32mSparkContext.scala\u001b[39m:\u001b[32m2061\u001b[39m)\r\n  org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(\u001b[32mFileFormatWriter.scala\u001b[39m:\u001b[32m167\u001b[39m)\r\n  org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(\u001b[32mInsertIntoHadoopFsRelationCommand.scala\u001b[39m:\u001b[32m170\u001b[39m)\r\n  org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(\u001b[32mcommands.scala\u001b[39m:\u001b[32m104\u001b[39m)\r\n  org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(\u001b[32mcommands.scala\u001b[39m:\u001b[32m102\u001b[39m)\r\n  org.apache.spark.sql.execution.command.DataWritingCommandExec.doExecute(\u001b[32mcommands.scala\u001b[39m:\u001b[32m122\u001b[39m)\r\n  org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(\u001b[32mSparkPlan.scala\u001b[39m:\u001b[32m131\u001b[39m)\r\n  org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(\u001b[32mSparkPlan.scala\u001b[39m:\u001b[32m155\u001b[39m)\r\n  org.apache.spark.rdd.RDDOperationScope$.withScope(\u001b[32mRDDOperationScope.scala\u001b[39m:\u001b[32m151\u001b[39m)\r\n  org.apache.spark.sql.execution.SparkPlan.executeQuery(\u001b[32mSparkPlan.scala\u001b[39m:\u001b[32m152\u001b[39m)\r\n  org.apache.spark.sql.execution.SparkPlan.execute(\u001b[32mSparkPlan.scala\u001b[39m:\u001b[32m127\u001b[39m)\r\n  org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(\u001b[32mQueryExecution.scala\u001b[39m:\u001b[32m83\u001b[39m)\r\n  org.apache.spark.sql.execution.QueryExecution.toRdd(\u001b[32mQueryExecution.scala\u001b[39m:\u001b[32m81\u001b[39m)\r\n  org.apache.spark.sql.DataFrameWriter.$anonfun$runCommand$1(\u001b[32mDataFrameWriter.scala\u001b[39m:\u001b[32m676\u001b[39m)\r\n  org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(\u001b[32mSQLExecution.scala\u001b[39m:\u001b[32m80\u001b[39m)\r\n  org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(\u001b[32mSQLExecution.scala\u001b[39m:\u001b[32m127\u001b[39m)\r\n  org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(\u001b[32mSQLExecution.scala\u001b[39m:\u001b[32m75\u001b[39m)\r\n  org.apache.spark.sql.DataFrameWriter.runCommand(\u001b[32mDataFrameWriter.scala\u001b[39m:\u001b[32m676\u001b[39m)\r\n  org.apache.spark.sql.DataFrameWriter.saveToV1Source(\u001b[32mDataFrameWriter.scala\u001b[39m:\u001b[32m290\u001b[39m)\r\n  org.apache.spark.sql.DataFrameWriter.save(\u001b[32mDataFrameWriter.scala\u001b[39m:\u001b[32m271\u001b[39m)\r\n  org.apache.spark.sql.DataFrameWriter.save(\u001b[32mDataFrameWriter.scala\u001b[39m:\u001b[32m229\u001b[39m)\r\n  ammonite.$sess.cmd31$Helper.<init>(\u001b[32mcmd31.sc\u001b[39m:\u001b[32m1\u001b[39m)\r\n  ammonite.$sess.cmd31$.<init>(\u001b[32mcmd31.sc\u001b[39m:\u001b[32m7\u001b[39m)\r\n  ammonite.$sess.cmd31$.<clinit>(\u001b[32mcmd31.sc\u001b[39m:\u001b[32m-1\u001b[39m)\r\n\u001b[31mjava.io.IOException: (null) entry in command string: null chmod 0644 C:\\Users\\KH\\Desktop\\Master 1\\BigData\\bigdata\\bigdata\\data\\parquet_data_1\\_temporary\\0\\_temporary\\attempt_20230425190851_0048_m_000000_1423\\part-00000-4560d31a-9d8e-40ee-8898-bf5b767531ad-c000.snappy.parquet\u001b[39m\r\n  org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(\u001b[32mShell.java\u001b[39m:\u001b[32m762\u001b[39m)\r\n  org.apache.hadoop.util.Shell.execCommand(\u001b[32mShell.java\u001b[39m:\u001b[32m859\u001b[39m)\r\n  org.apache.hadoop.util.Shell.execCommand(\u001b[32mShell.java\u001b[39m:\u001b[32m842\u001b[39m)\r\n  org.apache.hadoop.fs.RawLocalFileSystem.setPermission(\u001b[32mRawLocalFileSystem.java\u001b[39m:\u001b[32m661\u001b[39m)\r\n  org.apache.hadoop.fs.ChecksumFileSystem$1.apply(\u001b[32mChecksumFileSystem.java\u001b[39m:\u001b[32m501\u001b[39m)\r\n  org.apache.hadoop.fs.ChecksumFileSystem$FsOperation.run(\u001b[32mChecksumFileSystem.java\u001b[39m:\u001b[32m482\u001b[39m)\r\n  org.apache.hadoop.fs.ChecksumFileSystem.setPermission(\u001b[32mChecksumFileSystem.java\u001b[39m:\u001b[32m498\u001b[39m)\r\n  org.apache.hadoop.fs.ChecksumFileSystem.create(\u001b[32mChecksumFileSystem.java\u001b[39m:\u001b[32m467\u001b[39m)\r\n  org.apache.hadoop.fs.ChecksumFileSystem.create(\u001b[32mChecksumFileSystem.java\u001b[39m:\u001b[32m433\u001b[39m)\r\n  org.apache.hadoop.fs.FileSystem.create(\u001b[32mFileSystem.java\u001b[39m:\u001b[32m908\u001b[39m)\r\n  org.apache.hadoop.fs.FileSystem.create(\u001b[32mFileSystem.java\u001b[39m:\u001b[32m889\u001b[39m)\r\n  org.apache.parquet.hadoop.util.HadoopOutputFile.create(\u001b[32mHadoopOutputFile.java\u001b[39m:\u001b[32m74\u001b[39m)\r\n  org.apache.parquet.hadoop.ParquetFileWriter.<init>(\u001b[32mParquetFileWriter.java\u001b[39m:\u001b[32m248\u001b[39m)\r\n  org.apache.parquet.hadoop.ParquetOutputFormat.getRecordWriter(\u001b[32mParquetOutputFormat.java\u001b[39m:\u001b[32m390\u001b[39m)\r\n  org.apache.parquet.hadoop.ParquetOutputFormat.getRecordWriter(\u001b[32mParquetOutputFormat.java\u001b[39m:\u001b[32m349\u001b[39m)\r\n  org.apache.spark.sql.execution.datasources.parquet.ParquetOutputWriter.<init>(\u001b[32mParquetOutputWriter.scala\u001b[39m:\u001b[32m37\u001b[39m)\r\n  org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$$anon$1.newInstance(\u001b[32mParquetFileFormat.scala\u001b[39m:\u001b[32m151\u001b[39m)\r\n  org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.newOutputWriter(\u001b[32mFileFormatDataWriter.scala\u001b[39m:\u001b[32m123\u001b[39m)\r\n  org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.<init>(\u001b[32mFileFormatDataWriter.scala\u001b[39m:\u001b[32m108\u001b[39m)\r\n  org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(\u001b[32mFileFormatWriter.scala\u001b[39m:\u001b[32m236\u001b[39m)\r\n  org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$write$15(\u001b[32mFileFormatWriter.scala\u001b[39m:\u001b[32m177\u001b[39m)\r\n  org.apache.spark.scheduler.ResultTask.runTask(\u001b[32mResultTask.scala\u001b[39m:\u001b[32m90\u001b[39m)\r\n  org.apache.spark.scheduler.Task.run(\u001b[32mTask.scala\u001b[39m:\u001b[32m123\u001b[39m)\r\n  org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(\u001b[32mExecutor.scala\u001b[39m:\u001b[32m411\u001b[39m)\r\n  org.apache.spark.util.Utils$.tryWithSafeFinally(\u001b[32mUtils.scala\u001b[39m:\u001b[32m1360\u001b[39m)\r\n  org.apache.spark.executor.Executor$TaskRunner.run(\u001b[32mExecutor.scala\u001b[39m:\u001b[32m414\u001b[39m)\r\n  java.util.concurrent.ThreadPoolExecutor.runWorker(\u001b[32mThreadPoolExecutor.java\u001b[39m:\u001b[32m1128\u001b[39m)\r\n  java.util.concurrent.ThreadPoolExecutor$Worker.run(\u001b[32mThreadPoolExecutor.java\u001b[39m:\u001b[32m628\u001b[39m)\r\n  java.lang.Thread.run(\u001b[32mThread.java\u001b[39m:\u001b[32m829\u001b[39m)"
     ]
    }
   ],
   "source": [
    "dfWithEncondedDate.write.format(\"parquet\").save(\"./data/parquet_data_1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q18. Rechargez  les données stockées en format Parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "cmd8.sc:1: not found: value sparkSession\n",
      "val newdataDF = sparkSession.read.parquet(\"data/parquet_data\")\n",
      "                ^Compilation Failed"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "Compilation Failed"
     ]
    }
   ],
   "source": [
    "val newdataDF = sparkSession.read.parquet(\"data/parquet_data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- CallNumber: integer (nullable = true)\n",
      " |-- UnitID: string (nullable = true)\n",
      " |-- IncidentNumber: integer (nullable = true)\n",
      " |-- CallType: string (nullable = true)\n",
      " |-- CallFinalDisposition: string (nullable = true)\n",
      " |-- Address: string (nullable = true)\n",
      " |-- City: string (nullable = true)\n",
      " |-- Zipcode: integer (nullable = true)\n",
      " |-- Battalion: string (nullable = true)\n",
      " |-- StationArea: string (nullable = true)\n",
      " |-- Box: string (nullable = true)\n",
      " |-- OriginalPriority: string (nullable = true)\n",
      " |-- Priority: string (nullable = true)\n",
      " |-- FinalPriority: integer (nullable = true)\n",
      " |-- ALSUnit: boolean (nullable = true)\n",
      " |-- CallTypeGroup: string (nullable = true)\n",
      " |-- NumAlarms: integer (nullable = true)\n",
      " |-- UnitType: string (nullable = true)\n",
      " |-- UnitSequenceInCallDispatch: integer (nullable = true)\n",
      " |-- FirePreventionDistrict: string (nullable = true)\n",
      " |-- SupervisorDistrict: string (nullable = true)\n",
      " |-- Neighborhood: string (nullable = true)\n",
      " |-- Location: string (nullable = true)\n",
      " |-- RowID: string (nullable = true)\n",
      " |-- ResponseDelayedinMins: float (nullable = true)\n",
      " |-- IncidentDate: timestamp (nullable = true)\n",
      " |-- OnWatchDate: timestamp (nullable = true)\n",
      " |-- AvailableDtTS: timestamp (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfWithEncondedDate.printSchema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Scala",
   "language": "scala",
   "name": "scala"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".sc",
   "mimetype": "text/x-scala",
   "name": "scala",
   "nbconvert_exporter": "script",
   "version": "2.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
